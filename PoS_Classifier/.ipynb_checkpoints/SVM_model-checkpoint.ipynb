{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = json.load(open('./corpus/fr/fr.ftb.train.json', encoding = 'utf-8'))\n",
    "test_set = json.load(open('./corpus/fr/fr.ftb.test.json', encoding = 'utf-8'))\n",
    "\n",
    "train_set = train_set[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_and_labels(data_set):\n",
    "    \n",
    "    words = []\n",
    "    labels = []\n",
    "    for sentence,label in data_set:\n",
    "        for w,l in zip(sentence,label):\n",
    "            words.append(w)\n",
    "            labels.append(l)\n",
    "    \n",
    "    return words,labels\n",
    "\n",
    "train_words,train_label = words_and_labels(train_set)\n",
    "test_words,test_label = words_and_labels(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_and_labels(data_set):\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for sentence,label in data_set:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return sentences,labels\n",
    "\n",
    "train_sentences,_ = sentences_and_labels(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'X': 1, 'ADJ': 2, 'ADP+DET': 3, 'PUNCT': 4, 'VERB': 5, 'INTJ': 6, 'ADP+PRON': 7, 'PROPN': 8, 'SCONJ': 9, 'ADV': 10, 'CCONJ': 11, 'PRON': 12, 'ADP': 13, 'DET': 14, 'NUM': 15, 'AUX': 16, 'NOUN': 17})\n"
     ]
    }
   ],
   "source": [
    "def train_one_hot(train_data):\n",
    "    \n",
    "    words_set = set()\n",
    "    words_dict = defaultdict(int)\n",
    "    for word in train_data:\n",
    "        words_set.add(word)\n",
    "    words_set = list(words_set)\n",
    "    \n",
    "    for id,word in enumerate(words_set):\n",
    "        words_dict[word] = id+1\n",
    "    \n",
    "    return words_dict\n",
    "    \n",
    "train_words_dict = train_one_hot(train_words)\n",
    "train_labels_dict = train_one_hot(train_label)\n",
    "\n",
    "print(train_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'DET': 14, 'NOUN': 17, 'ADP+DET': 3, 'VERB': 5, 'ADP': 13, 'NUM': 15, 'ADJ': 2, 'PUNCT': 4, 'AUX': 16, 'ADV': 10, 'CCONJ': 11, 'PRON': 12, 'SCONJ': 9, 'PROPN': 8, 'PART': 0, 'X': 1, 'ADP+PRON': 7, 'INTJ': 6, 'ADP+ADP': 0})\n"
     ]
    }
   ],
   "source": [
    "def test_one_hot(test_data,train_dict):\n",
    "    words_dict = defaultdict(int)\n",
    "    for word in test_data:\n",
    "        if word in train_dict:\n",
    "            words_dict[word] = train_dict[word]\n",
    "        else:\n",
    "            words_dict[word] = 0\n",
    "    return words_dict\n",
    "\n",
    "test_words_dict = test_one_hot(test_words,train_words_dict)\n",
    "test_labels_dict = test_one_hot(test_label,train_labels_dict)\n",
    "\n",
    "print(test_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(76, 528), (528, 775), (781, 528), (528, 657)]\n"
     ]
    }
   ],
   "source": [
    "def feature_window(i, sentence,words_dict,l=2):\n",
    "    '''\n",
    "    i : the index of the word in the context\n",
    "    context : the sentence\n",
    "    l : a window of size is 2*l+1\n",
    "    \n",
    "    return : list of features which are tuple (feature_name, value)\n",
    "    '''\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    word = words_dict[sentence[i]]\n",
    "    \n",
    "    for k in range(1,l+1):\n",
    "        \n",
    "        if i-k >= 0:\n",
    "            res.append((words_dict[sentence[i-k]],word))\n",
    "            \n",
    "        if i+k<len(sentence):\n",
    "            res.append((word,words_dict[sentence[i+k]]))\n",
    "        \n",
    "    return res\n",
    "\n",
    "print(feature_window(7,train_sentences[4],train_words_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_suffix(i,sentence):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for k in range(1,len(sentence[i])):\n",
    "        res.append('suffix_'+sentence[i][k:])\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_shape(i, sentence):\n",
    "    '''\n",
    "    i : the index of the word in the context\n",
    "    context : the sentence\n",
    "    \n",
    "    return : list of features which are tuple (feature_name, value)\n",
    "    '''\n",
    "    def has_digit(s):\n",
    "        '''\n",
    "        check if a string has digit or nor\n",
    "        '''\n",
    "        return any(c.isdigit() for c in s)\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    word = sentence[i]\n",
    "    \n",
    "       ## different orthographic\n",
    "    # banary feature indicating whether the word starts with a capital letter or not, 1:yes, 0:not\n",
    "    if word.istitle():\n",
    "        res.append('start_capital')\n",
    "    # banary feature indicating whether the word is made of all capital letters or not, 1:yes, 0:not\n",
    "    if word.isupper():\n",
    "        res.append('only_capital')\n",
    "    # banary feature indicating whether the word has a digit or not, 1:yes, 0:not\n",
    "    if has_digit(word):\n",
    "        res.append('has_digit')\n",
    "    # banary feature indicating whether the word has a hyphen or not, 1:yes, 0:not\n",
    "    if '-' in word:\n",
    "        res.append('has_hyphen')\n",
    "    # banary feature indicating whether the word has a low hyphen or not, 1:yes, 0:not\n",
    "    if '_' in word:\n",
    "        res.append('has_hyphen_low')\n",
    "    # banary feature indicating whether the letters in the word are all alphanumeric or not, 1:yes, 0:not\n",
    "    if not word.isalnum():\n",
    "        res.append('not_alnum')\n",
    "    # binary feature indicating whether the length of word is more than 3\n",
    "    if len(word) > 3:\n",
    "        res.append('word_len_>_3')\n",
    "    \n",
    "    if '\\'' in word:\n",
    "        res.append('abbr')\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_features_and_labels(data_set,words_dict,labels_dict):\n",
    "    \n",
    "    data = []\n",
    "    label = []\n",
    "    \n",
    "    #punct = pick_out_punct(data_set)\n",
    "    \n",
    "    #bigram_left,bigram_right = get_bigram(data_set)\n",
    "    \n",
    "    for sentence,labels in data_set:\n",
    "        \n",
    "        for i in range(len(sentence)):\n",
    "              \n",
    "            data_of_word = []\n",
    "\n",
    "            data_of_word += feature_window(i, sentence, words_dict)\n",
    "            #data_of_word += feature_suffix(i, sentence)\n",
    "            #data_of_word += feature_shape(i, sentence)\n",
    "            #data_of_word += feature_distributional(i, sentence, bigram_left,'_left_')\n",
    "\n",
    "            data += data_of_word\n",
    "            label.append(labels_dict[labels[i]])\n",
    "            \n",
    "    return data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_labels = collect_features_and_labels(train_set,train_words_dict,train_labels_dict)\n",
    "#test_data,test_labels = collect_features_and_labels(test_set,test_words_dict,test_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17168\n",
      "4442\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372\n"
     ]
    }
   ],
   "source": [
    "print(len(train_words_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss: 9.017371\n",
      "1: loss: 1.463558\n",
      "2: loss: 9.417523e-06\n",
      "3: loss: 9.417523e-06\n",
      "4: loss: 9.417523e-06\n",
      "5: loss: 9.417523e-06\n",
      "6: loss: 9.417523e-06\n",
      "7: loss: 9.417523e-06\n",
      "8: loss: 9.417523e-06\n",
      "9: loss: 9.417523e-06\n"
     ]
    }
   ],
   "source": [
    "word_size = len(train_words_dict)\n",
    "batch_size = len(train_data)\n",
    "embedding_size = 20\n",
    "\n",
    "context_pair = train_data\n",
    "\n",
    "inputs = [train_words_dict[x[0]] for x in context_pair]\n",
    "labels = [train_words_dict[x[1]] for x in context_pair]\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size,])\n",
    "\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([word_size, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([word_size,embedding_size],\n",
    "                        stddev=1.0 / np.sqrt(embedding_size)))\n",
    "\n",
    "nce_biases = tf.Variable(tf.zeros([word_size]))\n",
    "\n",
    "prediction = tf.add(tf.matmul(embed, tf.transpose(nce_weights)), nce_biases)\n",
    "train_labels_vector = tf.one_hot(train_labels,word_size)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=train_labels_vector))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "session = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "for iteration in range(0,10):\n",
    "    total_loss = 0\n",
    "\n",
    "    feed_dict = {train_inputs: inputs, train_labels: labels}\n",
    "    _, cur_loss,pred= session.run([optimizer, loss, prediction], feed_dict=feed_dict)\n",
    "    print('%s: loss: %s' %(iteration,cur_loss))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19.70553     0.32988912 -0.9683957  ... -0.07941938  0.59390706\n",
      "  -0.46528682]\n",
      " [19.70553     0.32988912 -0.9683957  ... -0.07941938  0.59390706\n",
      "  -0.46528682]]\n"
     ]
    }
   ],
   "source": [
    "print(pred[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = svm.SVC(C=1)\n",
    "clf.fit(train_data[:1000],train_labels[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[200, -1, 548, -1, 25196], [548, 200, 25196, -1, 12252], [25196, 548, 12252, 200, 14927], [12252, 25196, 14927, 548, 6316], [14927, 12252, 6316, 25196, 1], [6316, 14927, 1, 12252, 2608], [1, 6316, 2608, 14927, 16389], [2608, 1, 16389, 6316, 6688], [16389, 2608, 6688, 1, 1489], [6688, 16389, 1489, 2608, 18644]]\n",
      "[[26658, -1, 17841, -1, 19210], [17841, 26658, 19210, -1, 10374], [19210, 17841, 10374, 26658, 19210], [10374, 19210, 19210, 17841, 18832], [19210, 10374, 18832, 19210, 20491], [18832, 19210, 20491, 10374, 6012], [20491, 18832, 6012, 19210, 16471], [6012, 20491, 16471, 18832, 21630], [16471, 6012, 21630, 20491, 19950], [21630, 16471, 19950, 6012, 21630]]\n"
     ]
    }
   ],
   "source": [
    "train_hat = clf.predict(train_data[:1000])\n",
    "test_hat = clf.predict(test_data[:1000])\n",
    "print(train_data[:10])\n",
    "print(test_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0\n",
      "test accuracy 0.275\n"
     ]
    }
   ],
   "source": [
    "print('train accuracy:', accuracy_score(train_hat,train_labels[:1000]))\n",
    "print('test accuracy', accuracy_score(test_hat,test_labels[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.6080e+03,  1.0000e+00,  1.6389e+04,  6.3160e+03,  6.6880e+03],\n",
       "       [ 1.2002e+04,  1.9286e+04,  2.3506e+04,  1.4503e+04,  1.6160e+04],\n",
       "       [ 7.4520e+03,  1.9220e+03,  8.3860e+03,  1.9286e+04, -1.0000e+00],\n",
       "       [ 1.3742e+04,  2.0916e+04,  1.2252e+04,  9.6400e+03,  1.1479e+04],\n",
       "       [ 1.9106e+04,  8.9810e+03,  2.4400e+04,  1.9945e+04,  2.4867e+04],\n",
       "       [ 4.5150e+03,  1.5617e+04,  1.0869e+04,  3.1080e+03,  5.4800e+02],\n",
       "       [ 1.4092e+04,  2.2350e+04,  2.4400e+04,  1.9391e+04,  1.9490e+03],\n",
       "       [ 4.4240e+03,  2.6975e+04,  2.1630e+04,  1.7663e+04,  2.2514e+04],\n",
       "       [ 1.9210e+04,  5.8480e+03,  1.0374e+04,  1.9286e+04,  1.9210e+04],\n",
       "       [ 1.9210e+04,  1.0374e+04,  1.3138e+04,  1.9210e+04,  3.6580e+03],\n",
       "       [ 1.9490e+03,  2.4400e+04,  8.6420e+03,  1.4092e+04,  1.5671e+04],\n",
       "       [ 1.9210e+04,  1.5671e+04,  8.1590e+03,  8.6420e+03,  6.6880e+03],\n",
       "       [ 5.4800e+02,  2.0000e+02,  2.5196e+04, -1.0000e+00,  1.2252e+04],\n",
       "       [ 5.4800e+02,  5.2490e+03,  8.0770e+03,  6.6880e+03,  1.7663e+04],\n",
       "       [ 5.4800e+02,  2.2514e+04,  9.0410e+03,  2.1630e+04,  2.5300e+02],\n",
       "       [ 5.4800e+02,  4.1360e+03,  1.1108e+04,  1.6160e+04,  1.9391e+04],\n",
       "       [ 5.4800e+02,  7.8500e+03,  9.0410e+03,  1.9391e+04,  2.6160e+03],\n",
       "       [ 8.3860e+03,  7.4520e+03, -1.0000e+00,  1.9220e+03, -1.0000e+00],\n",
       "       [ 5.4800e+02,  2.4867e+04,  3.1080e+03,  2.4400e+04,  1.5617e+04],\n",
       "       [ 5.4800e+02,  1.0869e+04,  9.2630e+03,  4.5150e+03,  1.9286e+04],\n",
       "       [ 1.4927e+04,  1.2252e+04,  6.3160e+03,  2.5196e+04,  1.0000e+00],\n",
       "       [ 8.0770e+03,  5.4800e+02,  1.7663e+04,  5.2490e+03,  2.6975e+04],\n",
       "       [ 1.4503e+04,  2.1073e+04,  1.9286e+04,  2.5300e+02,  1.2002e+04],\n",
       "       [ 1.1108e+04,  5.4800e+02,  1.9391e+04,  4.1360e+03,  7.8500e+03],\n",
       "       [ 2.6160e+03,  9.0410e+03,  2.1073e+04,  5.4800e+02,  1.9286e+04],\n",
       "       [ 4.9920e+03,  4.1610e+03,  1.6921e+04, -1.0000e+00,  9.2630e+03],\n",
       "       [ 5.5400e+03,  4.9070e+03,  1.2748e+04,  1.0070e+04,  9.2630e+03],\n",
       "       [ 1.1479e+04,  1.2252e+04,  1.9945e+04,  1.3742e+04,  8.9810e+03],\n",
       "       [ 2.4867e+04,  2.4400e+04,  5.4800e+02,  1.9106e+04,  3.1080e+03],\n",
       "       [ 3.6580e+03,  1.3138e+04,  1.4496e+04,  1.9210e+04,  1.6471e+04],\n",
       "       [ 1.4540e+04,  1.5982e+04,  7.4400e+03,  2.1718e+04,  7.8100e+03],\n",
       "       [ 7.8100e+03,  7.4400e+03,  1.9391e+04,  1.4540e+04,  2.2350e+04],\n",
       "       [ 1.5671e+04,  8.6420e+03,  1.9210e+04,  1.9490e+03,  8.1590e+03],\n",
       "       [ 6.3160e+03,  1.4927e+04,  1.0000e+00,  1.2252e+04,  2.6080e+03],\n",
       "       [ 9.2630e+03,  1.6921e+04,  1.6471e+04,  4.9920e+03,  1.0070e+04],\n",
       "       [ 9.2630e+03,  1.2748e+04,  9.6400e+03,  5.5400e+03,  2.0916e+04],\n",
       "       [ 9.2630e+03,  5.4800e+02,  1.9286e+04,  1.0869e+04,  5.8480e+03],\n",
       "       [ 2.0000e+02, -1.0000e+00,  5.4800e+02, -1.0000e+00,  2.5196e+04],\n",
       "       [ 1.2252e+04,  2.5196e+04,  1.4927e+04,  5.4800e+02,  6.3160e+03],\n",
       "       [ 9.0410e+03,  5.4800e+02,  2.5300e+02,  2.2514e+04,  2.1073e+04],\n",
       "       [ 2.1073e+04,  2.5300e+02,  1.4503e+04,  9.0410e+03,  1.9286e+04],\n",
       "       [ 9.0410e+03,  5.4800e+02,  2.6160e+03,  7.8500e+03,  2.1073e+04],\n",
       "       [ 2.1073e+04,  2.6160e+03,  1.9286e+04,  9.0410e+03,  1.9220e+03],\n",
       "       [ 1.9220e+03,  1.9286e+04,  7.4520e+03,  2.1073e+04,  8.3860e+03],\n",
       "       [ 4.1610e+03, -1.0000e+00,  4.9920e+03, -1.0000e+00,  1.6921e+04],\n",
       "       [ 1.2748e+04,  5.5400e+03,  9.2630e+03,  4.9070e+03,  9.6400e+03],\n",
       "       [ 1.2252e+04,  1.3742e+04,  1.1479e+04,  2.0916e+04,  1.9945e+04],\n",
       "       [ 2.3506e+04,  1.2002e+04,  1.6160e+04,  1.9286e+04,  4.1360e+03],\n",
       "       [ 2.4400e+04,  1.9106e+04,  2.4867e+04,  8.9810e+03,  5.4800e+02],\n",
       "       [ 2.4400e+04,  1.4092e+04,  1.9490e+03,  2.2350e+04,  8.6420e+03],\n",
       "       [ 2.5196e+04,  5.4800e+02,  1.2252e+04,  2.0000e+02,  1.4927e+04],\n",
       "       [ 1.6921e+04,  4.9920e+03,  9.2630e+03,  4.1610e+03,  1.6471e+04],\n",
       "       [ 1.5982e+04,  2.1718e+04,  1.4540e+04,  1.5617e+04,  7.4400e+03],\n",
       "       [ 6.6880e+03,  1.6389e+04,  1.4890e+03,  2.6080e+03,  1.8644e+04],\n",
       "       [ 1.8644e+04,  1.4890e+03,  1.9355e+04,  6.6880e+03,  2.7920e+03],\n",
       "       [ 6.6880e+03,  2.7920e+03,  5.2490e+03,  1.9355e+04,  5.4800e+02],\n",
       "       [ 3.1080e+03,  5.4800e+02,  1.5617e+04,  2.4867e+04,  4.5150e+03],\n",
       "       [ 1.4496e+04,  3.6580e+03,  1.6471e+04,  1.3138e+04,  2.2917e+04],\n",
       "       [ 6.4000e+01,  2.2917e+04,  1.5617e+04,  1.6471e+04,  2.1718e+04],\n",
       "       [ 7.4400e+03,  1.4540e+04,  7.8100e+03,  1.5982e+04,  1.9391e+04],\n",
       "       [ 6.6880e+03,  8.1590e+03,  1.9286e+04,  1.9210e+04,  1.0736e+04],\n",
       "       [ 1.0000e+00,  6.3160e+03,  2.6080e+03,  1.4927e+04,  1.6389e+04],\n",
       "       [ 1.9355e+04,  1.8644e+04,  2.7920e+03,  1.4890e+03,  6.6880e+03],\n",
       "       [ 1.7663e+04,  8.0770e+03,  2.6975e+04,  5.4800e+02,  4.4240e+03],\n",
       "       [ 1.9286e+04,  1.4503e+04,  1.2002e+04,  2.1073e+04,  2.3506e+04],\n",
       "       [ 1.6160e+04,  2.3506e+04,  4.1360e+03,  1.2002e+04,  5.4800e+02],\n",
       "       [ 1.9391e+04,  1.1108e+04,  7.8500e+03,  5.4800e+02,  5.4800e+02],\n",
       "       [ 1.9286e+04,  2.1073e+04,  1.9220e+03,  2.6160e+03,  7.4520e+03],\n",
       "       [ 1.6471e+04,  9.2630e+03,  1.0070e+04,  1.6921e+04,  4.9070e+03],\n",
       "       [ 9.6400e+03,  9.2630e+03,  2.0916e+04,  1.2748e+04,  1.3742e+04],\n",
       "       [ 1.9945e+04,  1.1479e+04,  8.9810e+03,  1.2252e+04,  1.9106e+04],\n",
       "       [ 1.5617e+04,  3.1080e+03,  4.5150e+03,  5.4800e+02,  1.0869e+04],\n",
       "       [ 1.9286e+04,  9.2630e+03,  5.8480e+03,  5.4800e+02,  1.9210e+04],\n",
       "       [ 1.6471e+04,  1.4496e+04,  2.2917e+04,  3.6580e+03,  6.4000e+01],\n",
       "       [ 1.5617e+04,  6.4000e+01,  2.1718e+04,  2.2917e+04,  1.5982e+04],\n",
       "       [ 1.9391e+04,  7.8100e+03,  2.2350e+04,  7.4400e+03,  1.4092e+04],\n",
       "       [ 1.9286e+04,  6.6880e+03,  1.0736e+04,  8.1590e+03,  1.9498e+04],\n",
       "       [ 2.1630e+04,  4.4240e+03,  2.2514e+04,  2.6975e+04,  5.4800e+02],\n",
       "       [ 2.5300e+02,  9.0410e+03,  2.1073e+04,  5.4800e+02,  1.4503e+04],\n",
       "       [ 4.9070e+03,  1.0070e+04,  5.5400e+03,  1.6471e+04,  1.2748e+04],\n",
       "       [ 1.6389e+04,  2.6080e+03,  6.6880e+03,  1.0000e+00,  1.4890e+03],\n",
       "       [ 1.4890e+03,  6.6880e+03,  1.8644e+04,  1.6389e+04,  1.9355e+04],\n",
       "       [ 2.7920e+03,  1.9355e+04,  6.6880e+03,  1.8644e+04,  5.2490e+03],\n",
       "       [ 5.2490e+03,  6.6880e+03,  5.4800e+02,  2.7920e+03,  8.0770e+03],\n",
       "       [ 2.6975e+04,  1.7663e+04,  4.4240e+03,  8.0770e+03,  2.1630e+04],\n",
       "       [ 2.2514e+04,  2.1630e+04,  5.4800e+02,  4.4240e+03,  9.0410e+03],\n",
       "       [ 4.1360e+03,  1.6160e+04,  5.4800e+02,  2.3506e+04,  1.1108e+04],\n",
       "       [ 7.8500e+03,  1.9391e+04,  5.4800e+02,  1.1108e+04,  9.0410e+03],\n",
       "       [ 1.0070e+04,  1.6471e+04,  4.9070e+03,  9.2630e+03,  5.5400e+03],\n",
       "       [ 2.0916e+04,  9.6400e+03,  1.3742e+04,  9.2630e+03,  1.2252e+04],\n",
       "       [ 8.9810e+03,  1.9945e+04,  1.9106e+04,  1.1479e+04,  2.4400e+04],\n",
       "       [ 1.0869e+04,  4.5150e+03,  5.4800e+02,  1.5617e+04,  9.2630e+03],\n",
       "       [ 5.8480e+03,  1.9286e+04,  1.9210e+04,  9.2630e+03,  1.0374e+04],\n",
       "       [ 1.0374e+04,  1.9210e+04,  1.9210e+04,  5.8480e+03,  1.3138e+04],\n",
       "       [ 1.3138e+04,  1.9210e+04,  3.6580e+03,  1.0374e+04,  1.4496e+04],\n",
       "       [ 2.2917e+04,  1.6471e+04,  6.4000e+01,  1.4496e+04,  1.5617e+04],\n",
       "       [ 2.1718e+04,  1.5617e+04,  1.5982e+04,  6.4000e+01,  1.4540e+04],\n",
       "       [ 2.2350e+04,  1.9391e+04,  1.4092e+04,  7.8100e+03,  2.4400e+04],\n",
       "       [ 8.6420e+03,  1.9490e+03,  1.5671e+04,  2.4400e+04,  1.9210e+04],\n",
       "       [ 8.1590e+03,  1.9210e+04,  6.6880e+03,  1.5671e+04,  1.9286e+04]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
