{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = json.load(open('./corpus/fr/fr.ftb.train.json', encoding = 'utf-8'))\n",
    "test_set = json.load(open('./corpus/fr/fr.ftb.test.json', encoding = 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_out_punct(data_set):\n",
    "    \n",
    "    punct = set()\n",
    "    \n",
    "    for sentence,labels in data_set:\n",
    "        for word,label in zip(sentence,labels):\n",
    "            if label == 'PUNCT':\n",
    "                punct.add(word)\n",
    "    \n",
    "    return punct\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '(',\n",
       " '(*)',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '...',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '?',\n",
       " '[',\n",
       " '[â€¦]',\n",
       " ']'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pick_out_punct(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_window(i, sentence, l=2):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    word = sentence[i]\n",
    "    res.append(word)\n",
    "    \n",
    "    for k in range(1,l+1):\n",
    "        \n",
    "        res.append('win_-'+str(k)+'_'+(sentence[i-k] if i-k>=0 else 'none'))\n",
    "        res.append('win_+'+str(k)+'_'+(sentence[i+k] if i+k<len(sentence) else 'none'))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_suffix(i,sentence):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for k in range(1,len(sentence[i])):\n",
    "        res.append('suffix_'+sentence[i][k:])\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_shape(i, sentence):\n",
    "\n",
    "    def has_digit(s):\n",
    "\n",
    "        return any(c.isdigit() for c in s)\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    word = sentence[i]\n",
    "    \n",
    "    if word.istitle():\n",
    "        res.append('start_capital')\n",
    "    if word.isupper():\n",
    "        res.append('only_capital')\n",
    "    if has_digit(word):\n",
    "        res.append('has_digit')\n",
    "    if '-' in word:\n",
    "        res.append('has_hyphen')\n",
    "    if '_' in word:\n",
    "        res.append('has_hyphen_low')\n",
    "    if not word.isalnum():\n",
    "        res.append('not_alnum')\n",
    "    if len(word) > 3:\n",
    "         res.append('word_len_>_3')\n",
    "    \n",
    "    if '\\'' in word:\n",
    "        res.append('abbr')\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(data_set):\n",
    "    \n",
    "    bigram_left = defaultdict(lambda: defaultdict(int))\n",
    "    bigram_right = defaultdict(lambda: defaultdict(int))\n",
    "    for sentence,labels in data_set:\n",
    "        for i in range(1,len(sentence)):\n",
    "            bigram_left[sentence[i]][sentence[i-1]] += 1\n",
    "            bigram_right[sentence[i-1]][sentence[i]] += 1\n",
    "            \n",
    "            \n",
    "    return bigram_left,bigram_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_distributional(i,sentence,bigram,direction):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    word = sentence[i]\n",
    "    \n",
    "    bigram_words = bigram[word]\n",
    "\n",
    "    m = sorted(bigram_words.items(), key = lambda item: item[1])\n",
    "    \n",
    "    freq_level = 0\n",
    "    \n",
    "    for k in m:\n",
    "        freq_level += 1\n",
    "        res.append(str(freq_level)+'_freq_'+k[0])\n",
    "        \n",
    "    \n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_features_and_labels(data_set):\n",
    "    \n",
    "    data = []\n",
    "    label = []\n",
    "    \n",
    "    punct = pick_out_punct(data_set)\n",
    "    \n",
    "    bigram_left,bigram_right = get_bigram(data_set)\n",
    "    \n",
    "    for sentence,labels in data_set:\n",
    "        \n",
    "        for i in range(len(sentence)):\n",
    "            \n",
    "            if sentence[i] in punct:\n",
    "                data.append(list(sentence[i]))\n",
    "            \n",
    "            else:\n",
    "                data_of_word = []\n",
    "\n",
    "                data_of_word += feature_window(i, sentence)\n",
    "                data_of_word += feature_suffix(i, sentence)\n",
    "                data_of_word += feature_shape(i, sentence)\n",
    "                #data_of_word += feature_distributional(i, sentence, bigram_left,'_left_')\n",
    "\n",
    "                data.append(data_of_word)\n",
    "        \n",
    "        label += labels\n",
    "            \n",
    "    return data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oov_features_and_labels(train_data,test_data,test_label):\n",
    "    \n",
    "    words = []\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for word in train_data:\n",
    "        words.append(word[0])\n",
    "    \n",
    "    train_words = set(words)\n",
    "    \n",
    "    for word,label in zip(test_data,test_label):\n",
    "        if word[0] not in train_words:\n",
    "            data.append(word)\n",
    "            labels.append(label)\n",
    "            \n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ambiguous_features_and_labels(input_data,input_label):\n",
    "    \n",
    "    words = defaultdict(lambda: set())\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for word,label in zip(input_data,input_label):\n",
    "        words[word[0]].add(label)\n",
    "    \n",
    "    for word,label in zip(input_data,input_label):\n",
    "        if(len(words[word[0]]) > 1):\n",
    "            data.append(word)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return data,labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_perceptron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.labels = []\n",
    "        \n",
    "        self.weights = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        self.weights_average = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "    def fit(self,sentences,labels):\n",
    "        \n",
    "        self.labels = list(set(labels))\n",
    "        \n",
    "        for features,label in zip(sentences,labels):\n",
    "            \n",
    "            self.update(features,label)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def predict(self,features):\n",
    "        \n",
    "        label_volt = np.zeros(len(self.labels))\n",
    "        \n",
    "        for feature in features:\n",
    "            \n",
    "            for label in self.labels:\n",
    "                \n",
    "                label_volt[self.labels.index(label)] += self.sigmoid(self.weights[feature][label])\n",
    "         \n",
    "        label_predict = self.labels[np.argmax(label_volt)]\n",
    "        \n",
    "        return label_predict\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict_all(self,sentences):\n",
    "        \n",
    "        labels_predict = []\n",
    "        \n",
    "        for features in sentences:\n",
    "            \n",
    "            labels_predict.append(self.predict(features))\n",
    "            \n",
    "        return labels_predict\n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate(self,sentences,labels):\n",
    "        \n",
    "        labels_predict = self.predict_all(sentences)\n",
    "        \n",
    "        correct_num = 0.\n",
    "        \n",
    "        for label_predict,label_real in zip(labels_predict,labels):\n",
    "            if label_predict == label_real:\n",
    "                correct_num += 1.\n",
    "        \n",
    "        print('accuracy of simple perceptron model: '+str(correct_num/len(labels) * 100)+'%')\n",
    "    \n",
    "    \n",
    "    def update(self,features,label_real):\n",
    "        \n",
    "        label_predict = self.predict(features)\n",
    "        delta = 10**(-7)\n",
    "            \n",
    "        if (label_predict == label_real):\n",
    "            return \n",
    "        \n",
    "        for feature in features:\n",
    "            \n",
    "            self.weights_average[feature][label_predict] += np.log(0.9)**2\n",
    "            self.weights_average[feature][label_real] += np.log(1.1)**2\n",
    "            \n",
    "            self.weights[feature][label_predict] += np.log(0.9)/(delta+np.sqrt(self.weights_average[feature][label_predict]))\n",
    "            \n",
    "            self.weights[feature][label_real] += np.log(1.1)/(delta+np.sqrt(self.weights_average[feature][label_real]))\n",
    "            #print(self.weights[feature][label_real])\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        \n",
    "        z = 1/(1 + np.exp(-x))\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time =  3.002885341644287\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "train_data,train_label = collect_features_and_labels(train_set)\n",
    "end = time.time()\n",
    "print('total time = ',end - begin)\n",
    "\n",
    "test_data,test_label = collect_features_and_labels(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['La', 'win_-1_none', 'win_+1_limite', 'win_-2_none', 'win_+2_des', 'suffix_a', 'start_capital']\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_data,oov_label = oov_features_and_labels(train_data,test_data,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_data,ambiguous_label = ambiguous_features_and_labels(train_data,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212787\n",
      "442228\n"
     ]
    }
   ],
   "source": [
    "print(len(ambiguous_data))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = simple_perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.fit(train_data,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of simple perceptron model: 94.94225620396148%\n"
     ]
    }
   ],
   "source": [
    "p.evaluate(test_data,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of simple perceptron model: 80.37974683544303%\n"
     ]
    }
   ],
   "source": [
    "p.evaluate(oov_data,oov_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of simple perceptron model: 95.0946251415735%\n"
     ]
    }
   ],
   "source": [
    "p.evaluate(ambiguous_data,ambiguous_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
