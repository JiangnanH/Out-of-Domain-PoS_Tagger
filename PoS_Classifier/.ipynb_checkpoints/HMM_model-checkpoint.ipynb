{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov model\n",
    "\n",
    "\n",
    "Best performance: accuracy 94.63% on corpus $fr.ftb$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import Counter,defaultdict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data sets and choose one as our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_ftb = json.load(open('./corpus/fr/fr.ftb.train.json', encoding = 'utf-8'))\n",
    "test_set_ftb = json.load(open('./corpus/fr/fr.ftb.test.json', encoding = 'utf-8'))\n",
    "train_set_gsd = json.load(open('./corpus/fr/fr.gsd.train.json', encoding = 'utf-8'))\n",
    "test_set_gsd = json.load(open('./corpus/fr/fr.gsd.test.json', encoding = 'utf-8'))\n",
    "train_set_partut = json.load(open('./corpus/fr/fr.partut.train.json', encoding = 'utf-8'))\n",
    "test_set_partut = json.load(open('./corpus/fr/fr.partut.test.json', encoding = 'utf-8'))\n",
    "train_set_pud = json.load(open('./corpus/fr/fr.pud.train.json', encoding = 'utf-8'))\n",
    "test_set_pud = json.load(open('./corpus/fr/fr.pud.test.json', encoding = 'utf-8'))\n",
    "train_set_sequoia = json.load(open('./corpus/fr/fr.sequoia.train.json', encoding = 'utf-8'))\n",
    "test_set_sequoia = json.load(open('./corpus/fr/fr.sequoia.test.json', encoding = 'utf-8'))\n",
    "train_set_spoken = json.load(open('./corpus/fr/fr.spoken.train.json', encoding = 'utf-8'))\n",
    "test_set_spoken = json.load(open('./corpus/fr/fr.spoken.test.json', encoding = 'utf-8'))\n",
    "\n",
    "test_set_foot = json.load(open('./corpus/fr/fr.foot.test.json', encoding = 'utf-8'))\n",
    "test_set_natdis = json.load(open('./corpus/fr/fr.natdis.test.json', encoding = 'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**words_and_labels**: Seperate the words and the labels from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_and_labels(data_set):\n",
    "    \n",
    "    words = []\n",
    "    labels = []\n",
    "    for sentence,label in data_set:\n",
    "        for w,l in zip(sentence,label):\n",
    "            words.append(w)\n",
    "            labels.append(l)\n",
    "    \n",
    "    return words,labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ambiguous_words**: Collect ambiguous words from the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ambiguous_words(train_data,train_label):\n",
    "    \n",
    "    words = defaultdict(lambda: set())\n",
    "    ambiguous_words = []\n",
    "    \n",
    "    for word,label in zip(train_data,train_label):\n",
    "        words[word].add(label)\n",
    "    \n",
    "    for word,label in zip(train_data,train_label):\n",
    "        if(len(words[word]) > 1):\n",
    "            ambiguous_words.append(word)\n",
    "    \n",
    "    return ambiguous_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**replace_unknown**: Return a copy of the input sequence where each unknown word is replaced by the literal string value 'nan'. Pomegranate will ignore these values during computation.\n",
    "\n",
    "**simplify_decoding**: Using the viterbi algorithm to decode the input sentence. Return the PoS predicted of all the words, including OOV words and ambiguous words.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "\n",
    "    return [w if w in train_data else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):\n",
    "    \n",
    "    w_list = replace_unknown(X)\n",
    "    \n",
    "    id_oov = []\n",
    "    id_amb = []\n",
    "    \n",
    "    _, state_path = model.viterbi(w_list)\n",
    "    \n",
    "    for i in range(len(w_list)):\n",
    "        if w_list[i] == 'nan':\n",
    "            id_oov.append(i)\n",
    "            state_path_oov = state_path\n",
    "        if w_list[i] in ambiguous_word:\n",
    "            id_amb.append(i)\n",
    "            state_path_amb = state_path\n",
    "    \n",
    "    state = []\n",
    "    state_oov = []\n",
    "    state_amb = []\n",
    "    for i in range(len(state_path[1:-1])):\n",
    "        state.append(state_path[i+1][1].name)\n",
    "        if i in id_oov:\n",
    "            state_oov.append(state_path[i+1][1].name)\n",
    "        if i in id_amb:\n",
    "            state_amb.append(state_path[i+1][1].name)\n",
    "            \n",
    "    return state,state_oov,id_oov,state_amb,id_amb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**accuracy**: Calculate the prediction accuracy by using the model to decode each sentence and comparing the prediction with the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, model):\n",
    "    \n",
    "    correct = total_predictions = 0\n",
    "    correct_oov = total_predictions_oov = 0\n",
    "    correct_amb = total_predictions_amb = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        # The model.viterbi call in simplify_decoding will return None if the HMM\n",
    "        # raises an error (for example, if a test sentence contains a word that\n",
    "        # is out of vocabulary for the training set). Any exception counts the\n",
    "        # full sentence as an error (which makes this a conservative estimate).\n",
    "        try:\n",
    "            most_likely_tags,most_likely_tags_oov,id_oov,most_likely_tags_amb,id_amb = simplify_decoding(observations, model)\n",
    "            actual_tags_oov = list(actual_tags[i] for i in id_oov)\n",
    "            actual_tags_amb = list(actual_tags[i] for i in id_amb)\n",
    "\n",
    "            correct_oov += sum(p == t for p, t in zip(most_likely_tags_oov, actual_tags_oov))\n",
    "            total_predictions_oov += len(most_likely_tags_oov)\n",
    "            \n",
    "            correct_amb += sum(p == t for p, t in zip(most_likely_tags_amb, actual_tags_amb))\n",
    "            total_predictions_amb += len(most_likely_tags_amb)\n",
    "\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "            total_predictions += len(most_likely_tags)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return correct / total_predictions, correct_oov/total_predictions_oov, correct_amb/total_predictions_amb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pair_counts**: Return a dictionary keyed to each unique value in the first sequence list that counts the number of occurrences of the corresponding value from the second sequences list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(data_set):\n",
    "    \n",
    "    dict_label_word = defaultdict(list)\n",
    "    dict_word = defaultdict(list)\n",
    "\n",
    "    for sentence,labels in data_set:\n",
    "        for word,label in zip(sentence,labels):\n",
    "            dict_label_word[label].append(word)\n",
    "    \n",
    "    for k in dict_label_word.keys():\n",
    "        dict_word[k] = Counter(dict_label_word[k])\n",
    "        \n",
    "    return dict_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**starting_counts**: Return a dictionary keyed to each unique value in the input sequences list that counts the number of occurrences where that value is at the beginning of a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starting_counts(data_set):\n",
    "    \n",
    "    dict_starting = defaultdict(list)\n",
    "    \n",
    "    lab = set()\n",
    "    for _,labels in data_set:\n",
    "        for label in labels:\n",
    "            lab.add(label)\n",
    "            \n",
    "    for label in lab:\n",
    "        dict_starting[label] = len([labels[0] for _,labels in data_set if labels[0]==label])\n",
    "        \n",
    "    return dict_starting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ending_counts**: Return a dictionary keyed to each unique value in the input sequences list that counts the number of occurrences where that value is at the end of a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ending_counts(data_set):\n",
    "    \n",
    "    dict_ending = defaultdict(list)\n",
    "    \n",
    "    lab = set()\n",
    "    for _,labels in data_set:\n",
    "        for label in labels:\n",
    "            lab.add(label)\n",
    "            \n",
    "    for label in lab:\n",
    "        dict_ending[label] = len([labels[-1] for _,labels in data_set if labels[-1]==label])\n",
    "    return dict_ending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unigram_counts**: Return a dictionary keyed to each unique value in the input sequence list that counts the number of occurrences of the value in the sequences list. The sequences collection should be a 2-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_counts(data_set):\n",
    "    \n",
    "    dict_tag = defaultdict(int)\n",
    "    \n",
    "    for _,labels in data_set:\n",
    "        for label in labels:\n",
    "            dict_tag[label] += 1           \n",
    "    return dict_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bigram_counts**: Return a dictionary keyed to each unique PAIR of values in the input sequences list that counts the number of occurrences of pair in the sequences list. The input should be a 2-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_counts(data_set):\n",
    "    \n",
    "    dict_bigram = defaultdict(int)\n",
    "    for _,labels in data_set:\n",
    "        for i in range(1,len(labels)):\n",
    "            dict_bigram[(labels[i-1],labels[i])] += 1\n",
    "            \n",
    "    return dict_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the data and construct the HMM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set_ftb\n",
    "test_set = test_set_ftb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HiddenMarkovModel(name=\"base-hmm-tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_starts = starting_counts(train_set)\n",
    "label_ends = ending_counts(train_set)\n",
    "label_unigrams = unigram_counts(train_set)\n",
    "label_bigrams = bigram_counts(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212787\n"
     ]
    }
   ],
   "source": [
    "train_data,train_label = words_and_labels(train_set)\n",
    "ambiguous_word = ambiguous_words(train_data,train_label)\n",
    "print(len(ambiguous_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_label_and_word = pair_counts(train_set)\n",
    "states = {}\n",
    "for label, word_dict in count_label_and_word.items():\n",
    "    p_words_given_label_state = defaultdict(float)\n",
    "    # for each label/word, calculate P(word|label)\n",
    "    for word in word_dict.keys():\n",
    "        p_words_given_label_state[word] = count_label_and_word[label][word] / label_unigrams[label] \n",
    "    # create a new state for each label from the dict of words that represents P(word|label)\n",
    "    emission = DiscreteDistribution(dict(p_words_given_label_state))\n",
    "    states[label] = State(emission, name=label)\n",
    "    \n",
    "hmm.add_states(list(states.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = set()\n",
    "for _,labels in train_set:\n",
    "    for label in labels:\n",
    "        label_list.add(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding start states\n",
    "for label in label_list:        \n",
    "    state = states[label]\n",
    "    hmm.add_transition(hmm.start, state, label_starts[label]/len(label_list))\n",
    "\n",
    "# Adding end states\n",
    "for label in label_list: \n",
    "    state = states[label]\n",
    "    hmm.add_transition(state, hmm.end, label_ends[label]/label_unigrams[label])\n",
    "\n",
    "# Adding pairs\n",
    "for label1 in label_list: \n",
    "    state1 = states[label1]\n",
    "    for label2 in label_list: \n",
    "        state2 = states[label2]\n",
    "        hmm.add_transition(state1, state2, label_bigrams[(label1, label2)]/label_unigrams[label1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.bake()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the data from test set and prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_and_labels(data_set):\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for sentence,label in data_set:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return sentences,labels\n",
    "\n",
    "test_data,test_label = sentences_and_labels(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_training_acc = accuracy(test_data,test_label,hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.9462762554643352\n",
      "oov accuracy: 0.46537396121883656\n",
      "amb accuracy: 0.9329699059909885\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy:',hmm_training_acc[0])\n",
    "print('oov accuracy:',hmm_training_acc[1])\n",
    "print('amb accuracy:',hmm_training_acc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
