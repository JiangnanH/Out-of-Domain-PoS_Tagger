{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import Counter,defaultdict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_ftb = json.load(open('./corpus/fr/fr.ftb.train.json', encoding = 'utf-8'))\n",
    "test_set_ftb = json.load(open('./corpus/fr/fr.ftb.test.json', encoding = 'utf-8'))\n",
    "train_set_gsd = json.load(open('./corpus/fr/fr.gsd.train.json', encoding = 'utf-8'))\n",
    "test_set_gsd = json.load(open('./corpus/fr/fr.gsd.test.json', encoding = 'utf-8'))\n",
    "train_set_partut = json.load(open('./corpus/fr/fr.partut.train.json', encoding = 'utf-8'))\n",
    "test_set_partut = json.load(open('./corpus/fr/fr.partut.test.json', encoding = 'utf-8'))\n",
    "train_set_pud = json.load(open('./corpus/fr/fr.pud.train.json', encoding = 'utf-8'))\n",
    "test_set_pud = json.load(open('./corpus/fr/fr.pud.test.json', encoding = 'utf-8'))\n",
    "train_set_sequoia = json.load(open('./corpus/fr/fr.sequoia.train.json', encoding = 'utf-8'))\n",
    "test_set_sequoia = json.load(open('./corpus/fr/fr.sequoia.test.json', encoding = 'utf-8'))\n",
    "train_set_spoken = json.load(open('./corpus/fr/fr.spoken.train.json', encoding = 'utf-8'))\n",
    "test_set_spoken = json.load(open('./corpus/fr/fr.spoken.test.json', encoding = 'utf-8'))\n",
    "\n",
    "test_set_foot = json.load(open('./corpus/fr/fr.foot.test.json', encoding = 'utf-8'))\n",
    "test_set_natdis = json.load(open('./corpus/fr/fr.natdis.test.json', encoding = 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_and_labels(data_set):\n",
    "    \n",
    "    words = []\n",
    "    labels = []\n",
    "    for sentence,label in data_set:\n",
    "        for w,l in zip(sentence,label):\n",
    "            words.append(w)\n",
    "            labels.append(l)\n",
    "    \n",
    "    return words,labels\n",
    "\n",
    "train_data,train_label = words_and_labels(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212787\n"
     ]
    }
   ],
   "source": [
    "def ambiguous_words(train_data,train_label):\n",
    "    \n",
    "    words = defaultdict(lambda: set())\n",
    "    ambiguous_words = []\n",
    "    \n",
    "    for word,label in zip(train_data,train_label):\n",
    "        words[word].add(label)\n",
    "    \n",
    "    for word,label in zip(train_data,train_label):\n",
    "        if(len(words[word]) > 1):\n",
    "            ambiguous_words.append(word)\n",
    "    \n",
    "    return ambiguous_words\n",
    "\n",
    "ambiguous_words = ambiguous_words(train_data,train_label)\n",
    "print(len(ambiguous_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "    \"\"\"Return a copy of the input sequence where each unknown word is replaced\n",
    "    by the literal string value 'nan'. Pomegranate will ignore these values\n",
    "    during computation.\n",
    "    \"\"\"\n",
    "    return [w if w in train_data else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):\n",
    "    \n",
    "    \"\"\"X should be a 1-D sequence of observations for the model to predict\"\"\"\n",
    "    \n",
    "    w_list = replace_unknown(X)\n",
    "    \n",
    "    id_oov = []\n",
    "    id_amb = []\n",
    "    \n",
    "    _, state_path = model.viterbi(w_list)\n",
    "    \n",
    "    for i in range(len(w_list)):\n",
    "        if w_list[i] == 'nan':\n",
    "            id_oov.append(i)\n",
    "            state_path_oov = state_path\n",
    "        if w_list[i] in ambiguous_words:\n",
    "            id_amb.append(i)\n",
    "            state_path_amb = state_path\n",
    "    \n",
    "    state = []\n",
    "    state_oov = []\n",
    "    state_amb = []\n",
    "    for i in range(len(state_path[1:-1])):\n",
    "        state.append(state_path[i+1][1].name)\n",
    "        if i in id_oov:\n",
    "            state_oov.append(state_path[i+1][1].name)\n",
    "        if i in id_amb:\n",
    "            state_amb.append(state_path[i+1][1].name)\n",
    "            \n",
    "    return state,state_oov,id_oov,state_amb,id_amb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, model):\n",
    "    \"\"\"Calculate the prediction accuracy by using the model to decode each sequence\n",
    "    in the input X and comparing the prediction with the true labels in Y.\n",
    "    \n",
    "    The X should be an array whose first dimension is the number of sentences to test,\n",
    "    and each element of the array should be an iterable of the words in the sequence.\n",
    "    The arrays X and Y should have the exact same shape.\n",
    "    \n",
    "    X = [(\"See\", \"Spot\", \"run\"), (\"Run\", \"Spot\", \"run\", \"fast\"), ...]\n",
    "    Y = [(), (), ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    correct = total_predictions = 0\n",
    "    correct_oov = total_predictions_oov = 0\n",
    "    correct_amb = total_predictions_amb = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        # The model.viterbi call in simplify_decoding will return None if the HMM\n",
    "        # raises an error (for example, if a test sentence contains a word that\n",
    "        # is out of vocabulary for the training set). Any exception counts the\n",
    "        # full sentence as an error (which makes this a conservative estimate).\n",
    "        try:\n",
    "            most_likely_tags,most_likely_tags_oov,id_oov,most_likely_tags_amb,id_amb = simplify_decoding(observations, model)\n",
    "            actual_tags_oov = list(actual_tags[i] for i in id_oov)\n",
    "            actual_tags_amb = list(actual_tags[i] for i in id_amb)\n",
    "\n",
    "            correct_oov += sum(p == t for p, t in zip(most_likely_tags_oov, actual_tags_oov))\n",
    "            total_predictions_oov += len(most_likely_tags_oov)\n",
    "            \n",
    "            correct_amb += sum(p == t for p, t in zip(most_likely_tags_amb, actual_tags_amb))\n",
    "            total_predictions_amb += len(most_likely_tags_amb)\n",
    "\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "            total_predictions += len(most_likely_tags)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return correct / total_predictions, correct_oov/total_predictions_oov, correct_amb/total_predictions_amb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(data_set):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the first sequence list\n",
    "    that counts the number of occurrences of the corresponding value from the\n",
    "    second sequences list.\n",
    "    \n",
    "    For example, if sequences_A is tags and sequences_B is the corresponding\n",
    "    words, then if 1244 sequences contain the word \"time\" tagged as a NOUN, then\n",
    "    you should return a dictionary such that pair_counts[NOUN][time] == 1244\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_label_word = defaultdict(list)\n",
    "    dict_word = defaultdict(list)\n",
    "\n",
    "    for sentence,labels in data_set:\n",
    "        for word,label in zip(sentence,labels):\n",
    "            dict_label_word[label].append(word)\n",
    "    \n",
    "    for k in dict_label_word.keys():\n",
    "        dict_word[k] = Counter(dict_label_word[k])\n",
    "        \n",
    "    return dict_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starting_counts(data_set):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the beginning of\n",
    "    a sequence.\n",
    "    \n",
    "    For example, if 8093 sequences start with NOUN, then you should return a\n",
    "    dictionary such that your_starting_counts[NOUN] == 8093\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_starting = defaultdict(list)\n",
    "    \n",
    "    lab = set()\n",
    "    for _,labels in data_set:\n",
    "        for label in labels:\n",
    "            lab.add(label)\n",
    "            \n",
    "    for label in lab:\n",
    "        dict_starting[label] = len([labels[0] for _,labels in data_set if labels[0]==label])\n",
    "        \n",
    "    return dict_starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ending_counts(data_set):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the end of\n",
    "    a sequence.\n",
    "    \n",
    "    For example, if 18 sequences end with DET, then you should return a\n",
    "    dictionary such that your_starting_counts[DET] == 18\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_ending = defaultdict(list)\n",
    "    \n",
    "    lab = set()\n",
    "    for _,labels in data_set:\n",
    "        for label in labels:\n",
    "            lab.add(label)\n",
    "            \n",
    "    for label in lab:\n",
    "        dict_ending[label] = len([labels[-1] for _,labels in data_set if labels[-1]==label])\n",
    "    return dict_ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_counts(data_set):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequence list that\n",
    "    counts the number of occurrences of the value in the sequences list. The sequences\n",
    "    collection should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the tag NOUN appears 275558 times over all the input sequences,\n",
    "    then you should return a dictionary such that your_unigram_counts[NOUN] == 275558.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_tag = defaultdict(int)\n",
    "    \n",
    "    for _,labels in data_set:\n",
    "        for label in labels:\n",
    "            dict_tag[label] += 1           \n",
    "    return dict_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_counts(data_set):\n",
    "    \"\"\"Return a dictionary keyed to each unique PAIR of values in the input sequences\n",
    "    list that counts the number of occurrences of pair in the sequences list. The input\n",
    "    should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the pair of tags (NOUN, VERB) appear 61582 times, then you should\n",
    "    return a dictionary such that your_bigram_counts[(NOUN, VERB)] == 61582\n",
    "    \"\"\"\n",
    "\n",
    "    dict_bigram = defaultdict(int)\n",
    "    for _,labels in data_set:\n",
    "        for i in range(1,len(labels)):\n",
    "            dict_bigram[(labels[i-1],labels[i])] += 1\n",
    "            \n",
    "    return dict_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set_ftb\n",
    "test_set = test_set_foot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HiddenMarkovModel(name=\"base-hmm-tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_starts = starting_counts(train_set)\n",
    "label_ends = ending_counts(train_set)\n",
    "label_unigrams = unigram_counts(train_set)\n",
    "label_bigrams = bigram_counts(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_label_and_word = pair_counts(train_set)\n",
    "states = {}\n",
    "for label, word_dict in count_label_and_word.items(): #data.training_set.tagset\n",
    "    p_words_given_label_state = defaultdict(float)\n",
    "    # for each tag/word, calculate P(word|tag)\n",
    "    for word in word_dict.keys(): # data.training_set.vocab\n",
    "        p_words_given_label_state[word] = count_label_and_word[label][word] / label_unigrams[label] \n",
    "    # create a new state for each tag from the dict of words that represents P(word|tag)\n",
    "    emission = DiscreteDistribution(dict(p_words_given_label_state))\n",
    "    states[label] = State(emission, name=label)\n",
    "    \n",
    "hmm.add_states(list(states.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = set()\n",
    "for _,labels in train_set:\n",
    "    for label in labels:\n",
    "        label_list.add(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding start states\n",
    "for label in label_list:        \n",
    "    state = states[label]\n",
    "    hmm.add_transition(hmm.start, state, label_starts[label]/len(label_list))\n",
    "\n",
    "# Adding end states\n",
    "for label in label_list: \n",
    "    state = states[label]\n",
    "    hmm.add_transition(state, hmm.end, label_ends[label]/label_unigrams[label])\n",
    "\n",
    "# Adding pairs\n",
    "for label1 in label_list: \n",
    "    state1 = states[label1]\n",
    "    for label2 in label_list: \n",
    "        state2 = states[label2]\n",
    "        hmm.add_transition(state1, state2, label_bigrams[(label1, label2)]/label_unigrams[label1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.bake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_and_labels(data_set):\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for sentence,label in data_set:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return sentences,labels\n",
    "\n",
    "test_data,test_label = sentences_and_labels(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_training_acc = accuracy(test_data,test_label,hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6304284887087435, 0.14045669969840585, 0.8157717366883847)\n"
     ]
    }
   ],
   "source": [
    "print(hmm_training_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
