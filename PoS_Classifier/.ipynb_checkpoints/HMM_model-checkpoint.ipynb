{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import Counter,defaultdict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = json.load(open('./corpus/fr/fr.ftb.train.json', encoding = 'utf-8'))\n",
    "test_set = json.load(open('./corpus/fr/fr.ftb.test.json', encoding = 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_and_labels(data_set):\n",
    "    \n",
    "    words = []\n",
    "    labels = []\n",
    "    for sentence,label in data_set:\n",
    "        for w,l in zip(sentence,label):\n",
    "            words.append(w)\n",
    "            labels.append(l)\n",
    "    \n",
    "    return words,labels\n",
    "\n",
    "train_data,train_label = words_and_labels(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "    \"\"\"Return a copy of the input sequence where each unknown word is replaced\n",
    "    by the literal string value 'nan'. Pomegranate will ignore these values\n",
    "    during computation.\n",
    "    \"\"\"\n",
    "    return [w if w in train_data else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):\n",
    "    \n",
    "    \"\"\"X should be a 1-D sequence of observations for the model to predict\"\"\"\n",
    "    \n",
    "    w_list = replace_unknown(X)\n",
    "    \n",
    "    a = []\n",
    "    \n",
    "    for i in range(len(w_list)):\n",
    "        if w_list[i] == 'nan':\n",
    "            a.append(i)\n",
    "            _, state_path_oov = model.viterbi(w_list)\n",
    "            \n",
    "    print(a)      \n",
    "            \n",
    "    _, state_path = model.viterbi(w_list)\n",
    "   \n",
    "    \n",
    "    state = [state[1].name for state in state_path[1:-1]]\n",
    "    m = state\n",
    "    state_oov = [state_oov[1].name for state_oov in state_path_oov[i+1] for i in a]\n",
    "    print(m)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, model):\n",
    "    \"\"\"Calculate the prediction accuracy by using the model to decode each sequence\n",
    "    in the input X and comparing the prediction with the true labels in Y.\n",
    "    \n",
    "    The X should be an array whose first dimension is the number of sentences to test,\n",
    "    and each element of the array should be an iterable of the words in the sequence.\n",
    "    The arrays X and Y should have the exact same shape.\n",
    "    \n",
    "    X = [(\"See\", \"Spot\", \"run\"), (\"Run\", \"Spot\", \"run\", \"fast\"), ...]\n",
    "    Y = [(), (), ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    correct = total_predictions = 0\n",
    "    correct_oov = total_predictions_oov = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        # The model.viterbi call in simplify_decoding will return None if the HMM\n",
    "        # raises an error (for example, if a test sentence contains a word that\n",
    "        # is out of vocabulary for the training set). Any exception counts the\n",
    "        # full sentence as an error (which makes this a conservative estimate).\n",
    "        try:\n",
    "            \n",
    "            most_likely_tags = simplify_decoding(observations, model)\n",
    "            #correct_oov += sum(p == t for p, t in zip(most_likely_tags_oov, actual_tags))\n",
    "            #total_predictions_oov += len(most_likely_tags_oov)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "        \n",
    "    return correct / total_predictions, correct_oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(data_set):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the first sequence list\n",
    "    that counts the number of occurrences of the corresponding value from the\n",
    "    second sequences list.\n",
    "    \n",
    "    For example, if sequences_A is tags and sequences_B is the corresponding\n",
    "    words, then if 1244 sequences contain the word \"time\" tagged as a NOUN, then\n",
    "    you should return a dictionary such that pair_counts[NOUN][time] == 1244\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_label_word = defaultdict(list)\n",
    "    dict_word = defaultdict(list)\n",
    "\n",
    "    for sentence,labels in data_set:\n",
    "        for word,label in zip(sentence,labels):\n",
    "            dict_label_word[label].append(word)\n",
    "    \n",
    "    for k in dict_label_word.keys():\n",
    "        dict_word[k] = Counter(dict_label_word[k])\n",
    "        \n",
    "    return dict_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starting_counts(data_set):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the beginning of\n",
    "    a sequence.\n",
    "    \n",
    "    For example, if 8093 sequences start with NOUN, then you should return a\n",
    "    dictionary such that your_starting_counts[NOUN] == 8093\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_starting = defaultdict(list)\n",
    "    \n",
    "    lab = set()\n",
    "    for _,labels in data_set:\n",
    "        for label in labels:\n",
    "            lab.add(label)\n",
    "            \n",
    "    for label in lab:\n",
    "        dict_starting[label] = len([labels[0] for _,labels in data_set if labels[0]==label])\n",
    "    return dict_starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ending_counts(data_set):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the end of\n",
    "    a sequence.\n",
    "    \n",
    "    For example, if 18 sequences end with DET, then you should return a\n",
    "    dictionary such that your_starting_counts[DET] == 18\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_ending = defaultdict(list)\n",
    "    \n",
    "    lab = set()\n",
    "    for _,labels in data_set:\n",
    "        for label in labels:\n",
    "            lab.add(label)\n",
    "            \n",
    "    for label in lab:\n",
    "        dict_ending[label] = len([labels[-1] for _,labels in data_set if labels[-1]==label])\n",
    "    return dict_ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_counts(data_set):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequence list that\n",
    "    counts the number of occurrences of the value in the sequences list. The sequences\n",
    "    collection should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the tag NOUN appears 275558 times over all the input sequences,\n",
    "    then you should return a dictionary such that your_unigram_counts[NOUN] == 275558.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_tag = defaultdict(int)\n",
    "    \n",
    "    for _,labels in data_set:\n",
    "        for label in labels:\n",
    "            dict_tag[label] += 1           \n",
    "    return dict_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_counts(data_set):\n",
    "    \"\"\"Return a dictionary keyed to each unique PAIR of values in the input sequences\n",
    "    list that counts the number of occurrences of pair in the sequences list. The input\n",
    "    should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the pair of tags (NOUN, VERB) appear 61582 times, then you should\n",
    "    return a dictionary such that your_bigram_counts[(NOUN, VERB)] == 61582\n",
    "    \"\"\"\n",
    "\n",
    "    dict_bigram = defaultdict(int)\n",
    "    for _,labels in data_set:\n",
    "        for i in range(1,len(labels)):\n",
    "            dict_bigram[(labels[i-1],labels[i])] += 1\n",
    "            \n",
    "    return dict_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HiddenMarkovModel(name=\"base-hmm-tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_starts = starting_counts(train_set)\n",
    "label_ends = ending_counts(train_set)\n",
    "label_unigrams = unigram_counts(train_set)\n",
    "label_bigrams = bigram_counts(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_label_and_word = pair_counts(train_set)\n",
    "states = {}\n",
    "for label, word_dict in count_label_and_word.items(): #data.training_set.tagset\n",
    "    p_words_given_label_state = defaultdict(float)\n",
    "    # for each tag/word, calculate P(word|tag)\n",
    "    for word in word_dict.keys(): # data.training_set.vocab\n",
    "        p_words_given_label_state[word] = count_label_and_word[label][word] / label_unigrams[label] \n",
    "    # create a new state for each tag from the dict of words that represents P(word|tag)\n",
    "    emission = DiscreteDistribution(dict(p_words_given_label_state))\n",
    "    states[label] = State(emission, name=label)\n",
    "    \n",
    "hmm.add_states(list(states.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = set()\n",
    "for _,labels in train_set:\n",
    "    for label in labels:\n",
    "        label_list.add(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in label_list:        \n",
    "    state = states[label]\n",
    "    hmm.add_transition(hmm.start, state, label_starts[label]/len(label_list))\n",
    "\n",
    "# Adding end states\n",
    "for label in label_list: \n",
    "    state = states[label]\n",
    "    hmm.add_transition(state, hmm.end, label_ends[label]/label_unigrams[label])\n",
    "\n",
    "# Adding pairs\n",
    "for label1 in label_list: \n",
    "    state1 = states[label1]\n",
    "    for label2 in label_list: \n",
    "        state2 = states[label2]\n",
    "        hmm.add_transition(state1, state2, label_bigrams[(label1, label2)]/label_unigrams[label1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.bake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_and_labels(data_set):\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for sentence,label in data_set:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return sentences,labels\n",
    "\n",
    "test_data,test_label = sentences_and_labels(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[18]\n",
      "[]\n",
      "[]\n",
      "[12]\n",
      "[1, 3]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "hmm_training_acc = accuracy(test_data[:10],test_label[:10],hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0)\n"
     ]
    }
   ],
   "source": [
    "print(hmm_training_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "index = []\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
