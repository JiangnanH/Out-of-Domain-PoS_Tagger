{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_ftb = json.load(open('./corpus/fr/fr.ftb.train.json', encoding = 'utf-8'))\n",
    "test_set_ftb = json.load(open('./corpus/fr/fr.ftb.test.json', encoding = 'utf-8'))\n",
    "train_set_gsd = json.load(open('./corpus/fr/fr.gsd.train.json', encoding = 'utf-8'))\n",
    "test_set_gsd = json.load(open('./corpus/fr/fr.gsd.test.json', encoding = 'utf-8'))\n",
    "train_set_partut = json.load(open('./corpus/fr/fr.partut.train.json', encoding = 'utf-8'))\n",
    "test_set_partut = json.load(open('./corpus/fr/fr.partut.test.json', encoding = 'utf-8'))\n",
    "train_set_pud = json.load(open('./corpus/fr/fr.pud.train.json', encoding = 'utf-8'))\n",
    "test_set_pud = json.load(open('./corpus/fr/fr.pud.test.json', encoding = 'utf-8'))\n",
    "train_set_sequoia = json.load(open('./corpus/fr/fr.sequoia.train.json', encoding = 'utf-8'))\n",
    "test_set_sequoia = json.load(open('./corpus/fr/fr.sequoia.test.json', encoding = 'utf-8'))\n",
    "train_set_spoken = json.load(open('./corpus/fr/fr.spoken.train.json', encoding = 'utf-8'))\n",
    "test_set_spoken = json.load(open('./corpus/fr/fr.spoken.test.json', encoding = 'utf-8'))\n",
    "\n",
    "test_set_foot = json.load(open('./corpus/fr/fr.foot.test.json', encoding = 'utf-8'))\n",
    "test_set_natdis = json.load(open('./corpus/fr/fr.natdis.test.json', encoding = 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set_ftb\n",
    "test_set = test_set_natdis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_and_labels(data_set):\n",
    "    \n",
    "    words = []\n",
    "    labels = []\n",
    "    for sentence,label in data_set:\n",
    "        for w,l in zip(sentence,label):\n",
    "            words.append(w)\n",
    "            labels.append(l)\n",
    "    \n",
    "    return words,labels\n",
    "\n",
    "train_words,train_label = words_and_labels(train_set)\n",
    "test_words,test_label = words_and_labels(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffix_one_hot(train_words):\n",
    "    \n",
    "    suffix_dict = defaultdict(int)\n",
    "    \n",
    "    for word in train_words:\n",
    "        for k in range(1,len(word)):\n",
    "            suffix_dict[word[k:]] += 1\n",
    "    return suffix_dict\n",
    "\n",
    "suffix_dict = suffix_one_hot(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_hot(train_data):\n",
    "    \n",
    "    words_set = set()\n",
    "    words_dict = defaultdict(int)\n",
    "    for word in train_data:\n",
    "        words_set.add(word)\n",
    "    words_set = list(words_set)\n",
    "    \n",
    "    for id,word in enumerate(words_set):\n",
    "        words_dict[word] = id\n",
    "    \n",
    "    return words_dict\n",
    "    \n",
    "train_words_dict = train_one_hot(train_words)\n",
    "train_labels_dict = train_one_hot(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_hot(test_data,train_dict):\n",
    "    words_dict = defaultdict(int)\n",
    "    for word in test_data:\n",
    "        if word in train_dict:\n",
    "            words_dict[word] = train_dict[word]\n",
    "        else:\n",
    "            words_dict[word] = -1\n",
    "    return words_dict\n",
    "\n",
    "test_words_dict = test_one_hot(test_words,train_words_dict)\n",
    "test_labels_dict = test_one_hot(test_label,train_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_window(i, sentence, words_dict, l=2):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    word = words_dict[sentence[i]]\n",
    "    res.append(word)\n",
    "    \n",
    "    for k in range(1,l+1):\n",
    "        \n",
    "        res.append(words_dict[sentence[i-k]] if i-k>=0 else -1)\n",
    "        res.append(words_dict[sentence[i+k]] if i+k<len(sentence) else -1)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_suffix(i,sentence):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for k in range(1,10):\n",
    "        if sentence[i][-k:] in suffix_dict:\n",
    "            res.append(suffix_dict[sentence[i][-k:]])\n",
    "        else:\n",
    "            res.append(-1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jialinbao\n"
     ]
    }
   ],
   "source": [
    "x = 'jialinbao'\n",
    "print(x[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_shape(word):\n",
    "    '''\n",
    "    i : the index of the word in the context\n",
    "    context : the sentence\n",
    "    \n",
    "    return : list of features which are tuple (feature_name, value)\n",
    "    '''\n",
    "    def has_digit(s):\n",
    "        '''\n",
    "        check if a string has digit or nor\n",
    "        '''\n",
    "        return any(c.isdigit() for c in s)\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "       ## different orthographic\n",
    "    # banary feature indicating whether the word starts with a capital letter or not, 1:yes, 0:not\n",
    "    res.append(1 if word.istitle() else 0)\n",
    "    # banary feature indicating whether the word is made of all capital letters or not, 1:yes, 0:not\n",
    "    res.append(1 if word.isupper() else 0)\n",
    "    # banary feature indicating whether the word has a digit or not, 1:yes, 0:not\n",
    "    res.append(1 if has_digit(word) else 0)\n",
    "    # banary feature indicating whether the word has a hyphen or not, 1:yes, 0:not\n",
    "    res.append(1 if '-' in word else 0)\n",
    "    # banary feature indicating whether the word has a low hyphen or not, 1:yes, 0:not\n",
    "    res.append(1 if '_' in word else 0)\n",
    "    # banary feature indicating whether the letters in the word are all alphanumeric or not, 1:yes, 0:not\n",
    "    res.append(1 if not word.isalnum() else 0)\n",
    "    # binary feature indicating whether the length of word is more than 3\n",
    "    res.append(1 if len(word) > 3 else 0)\n",
    "\n",
    "    res.append(1 if '\\'' in word else 0)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_features_and_labels(data_set,words_dict,labels_dict):\n",
    "    \n",
    "    data = []\n",
    "    label = []\n",
    "    \n",
    "    for sentence,labels in data_set:\n",
    "        \n",
    "        for i in range(len(sentence)):\n",
    "            \n",
    "            data_of_word = []\n",
    "\n",
    "            data_of_word += feature_window(i, sentence, words_dict)\n",
    "            data_of_word += feature_suffix(i, sentence)\n",
    "            data_of_word += feature_shape(sentence[i])\n",
    "\n",
    "\n",
    "            data.append(np.array(data_of_word))\n",
    "        \n",
    "            label.append(labels_dict[labels[i]])\n",
    "            \n",
    "    return data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oov_features_and_labels(train_data,test_data,test_label):\n",
    "    \n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for word,label in zip(test_data,test_label):\n",
    "        if word[0] == -1:\n",
    "            data.append(word)\n",
    "            labels.append(label)\n",
    "            \n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ambiguous_features_and_labels(input_data,input_label):\n",
    "    \n",
    "    words = defaultdict(lambda: set())\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for word,label in zip(input_data,input_label):\n",
    "        words[word[0]].add(label)\n",
    "    \n",
    "    for word,label in zip(input_data,input_label):\n",
    "        if(len(words[word[0]]) > 1):\n",
    "            data.append(word)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return data,labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time =  4.211343050003052\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "train_data,train_label = collect_features_and_labels(train_set,train_words_dict,train_labels_dict)\n",
    "end = time.time()\n",
    "print('total time = ',end - begin)\n",
    "\n",
    "test_data,test_label = collect_features_and_labels(test_set,test_words_dict,test_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5009 17227 11960 11501 23709 97943 12341  1055   166     7     7    -1\n",
      "    -1    -1     0     0     0     0     0     0     1     0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_data,oov_label = oov_features_and_labels(train_data,test_data,test_label)\n",
    "ambiguous_data,ambiguous_label = ambiguous_features_and_labels(train_data,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)\n",
    "train_label = np.array(train_label)\n",
    "test_data = np.array(test_data)\n",
    "test_label = np.array(test_label)\n",
    "oov_data = np.array(oov_data)\n",
    "oov_label = np.array(oov_label)\n",
    "ambiguous_data = np.array(ambiguous_data)\n",
    "ambiguous_label = np.array(ambiguous_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442228, 22)\n",
      "5\n",
      "(12044, 22)\n",
      "(2235, 22)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_label[10])\n",
    "print(test_data.shape)\n",
    "print(oov_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "tagger = tree.DecisionTreeClassifier()\n",
    "tagger.fit(train_data,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9993668424432646\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_hat = tagger.predict(train_data)\n",
    "print('train accuracy:', accuracy_score(train_hat,train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.7467618731318498\n",
      "amb accuracy: 0.9986841301395292\n",
      "oov accuracy: 0.18165548098434003\n"
     ]
    }
   ],
   "source": [
    "test_hat = tagger.predict(test_data)\n",
    "print('test accuracy:', accuracy_score(test_hat,test_label))\n",
    "amb_hat = tagger.predict(ambiguous_data)\n",
    "print('amb accuracy:', accuracy_score(amb_hat,ambiguous_label))\n",
    "oov_hat = tagger.predict(oov_data)\n",
    "print('oov accuracy:', accuracy_score(oov_hat,oov_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
