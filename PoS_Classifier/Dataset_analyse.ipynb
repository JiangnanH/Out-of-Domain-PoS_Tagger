{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's begin！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_ftb = json.load(open('./corpus/fr/fr.ftb.train.json', encoding = 'utf-8'))\n",
    "test_set_ftb = json.load(open('./corpus/fr/fr.ftb.test.json', encoding = 'utf-8'))\n",
    "train_set_gsd = json.load(open('./corpus/fr/fr.gsd.train.json', encoding = 'utf-8'))\n",
    "test_set_gsd = json.load(open('./corpus/fr/fr.gsd.test.json', encoding = 'utf-8'))\n",
    "train_set_partut = json.load(open('./corpus/fr/fr.partut.train.json', encoding = 'utf-8'))\n",
    "test_set_partut = json.load(open('./corpus/fr/fr.partut.test.json', encoding = 'utf-8'))\n",
    "train_set_pud = json.load(open('./corpus/fr/fr.pud.train.json', encoding = 'utf-8'))\n",
    "test_set_pud = json.load(open('./corpus/fr/fr.pud.test.json', encoding = 'utf-8'))\n",
    "train_set_sequoia = json.load(open('./corpus/fr/fr.sequoia.train.json', encoding = 'utf-8'))\n",
    "test_set_sequoia = json.load(open('./corpus/fr/fr.sequoia.test.json', encoding = 'utf-8'))\n",
    "train_set_spoken = json.load(open('./corpus/fr/fr.spoken.train.json', encoding = 'utf-8'))\n",
    "test_set_spoken = json.load(open('./corpus/fr/fr.spoken.test.json', encoding = 'utf-8'))\n",
    "\n",
    "test_set_foot = json.load(open('./corpus/fr/fr.foot.test.json', encoding = 'utf-8'))\n",
    "test_set_natdis = json.load(open('./corpus/fr/fr.natdis.test.json', encoding = 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_sentence_label(data_set):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for sentence,label in data_set:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return sentences,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_data_set(train_set,test_set):\n",
    "    \n",
    "    difference_between_sentences = []\n",
    "    difference_between_words = []\n",
    "    \n",
    "    difference_between_sentences.append(len(train_set))\n",
    "    difference_between_sentences.append(len(test_set))\n",
    "    difference_between_sentences.append(round(len(test_set)/len(train_set),5))\n",
    "    \n",
    "    train_sentence, train_label = seperate_sentence_label(train_set)\n",
    "    test_sentence, test_label = seperate_sentence_label(test_set)\n",
    "    \n",
    "    train_words_num = np.sum(list(len(sentence) for sentence in train_sentence))\n",
    "    test_words_num = np.sum(list(len(sentence) for sentence in test_sentence))\n",
    "    \n",
    "    difference_between_words.append(train_words_num)\n",
    "    difference_between_words.append(test_words_num)\n",
    "    difference_between_words.append(round(test_words_num/train_words_num,5))\n",
    "    \n",
    "    print(difference_between_sentences)\n",
    "    print(difference_between_words,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14759, 2541, 0.17217]\n",
      "[442228, 75073, 0.16976] \n",
      "\n",
      "[14450, 416, 0.02879]\n",
      "[345009, 9742, 0.02824] \n",
      "\n",
      "[803, 110, 0.13699]\n",
      "[23324, 2515, 0.10783] \n",
      "\n",
      "[803, 1000, 1.24533]\n",
      "[23324, 24138, 1.0349] \n",
      "\n",
      "[2231, 456, 0.20439]\n",
      "[49173, 9740, 0.19808] \n",
      "\n",
      "[1153, 726, 0.62966]\n",
      "[14952, 10010, 0.66948] \n",
      "\n",
      "[14759, 743, 0.05034]\n",
      "[442228, 13985, 0.03162] \n",
      "\n",
      "[14759, 622, 0.04214]\n",
      "[442228, 12044, 0.02723] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_data_set(train_set_ftb,test_set_ftb)\n",
    "display_data_set(train_set_gsd,test_set_gsd)\n",
    "display_data_set(train_set_partut,test_set_partut)\n",
    "display_data_set(train_set_pud,test_set_pud)\n",
    "display_data_set(train_set_sequoia,test_set_sequoia)\n",
    "display_data_set(train_set_spoken,test_set_spoken)\n",
    "\n",
    "display_data_set(train_set_ftb,test_set_foot)\n",
    "display_data_set(train_set_ftb,test_set_natdis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OOV_words_percentage(train_set,test_set):\n",
    "    train_sentence, train_label = seperate_sentence_label(train_set)\n",
    "    test_sentence, test_label = seperate_sentence_label(test_set)\n",
    "    \n",
    "    train_words = set()\n",
    "    test_words = set()\n",
    "    OOV_words = set()\n",
    "    \n",
    "    for sentence in train_sentence:\n",
    "        for word in sentence:\n",
    "            train_words.add(word)\n",
    "    \n",
    "    for sentence in test_sentence:\n",
    "        for word in sentence:    \n",
    "            test_words.add(word)\n",
    "            \n",
    "            if word not in train_words:\n",
    "                OOV_words.add(word)\n",
    "            \n",
    "    print(round(len(list(OOV_words))/len(list(test_words)),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21\n",
      "0.18\n",
      "0.28\n",
      "0.73\n",
      "0.29\n",
      "0.61\n"
     ]
    }
   ],
   "source": [
    "OOV_words_percentage(train_set_ftb,test_set_ftb)\n",
    "OOV_words_percentage(train_set_gsd,test_set_gsd)\n",
    "OOV_words_percentage(train_set_partut,test_set_partut)\n",
    "OOV_words_percentage(train_set_pud,test_set_pud)\n",
    "OOV_words_percentage(train_set_sequoia,test_set_sequoia)\n",
    "OOV_words_percentage(train_set_spoken,test_set_spoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['euh', 'bon', 'pour', 'aller', 'du', 'CRDT', 'à', 'la', 'gare', 'euh', 'de', 'Grenoble', 'je', 'euh', 'ben', 'je', 'sors', 'déjà', 'du', 'CRDT'], ['INTJ', 'INTJ', 'ADP', 'VERB', 'ADP', 'PROPN', 'ADP', 'DET', 'NOUN', 'INTJ', 'ADP', 'PROPN', 'PRON', 'INTJ', 'INTJ', 'PRON', 'VERB', 'ADV', 'ADP', 'PROPN']]\n",
      "[['euh', 'il', 'y', 'a', 'une', 'petite', 'bifurcation', 'euh', 'juste', 'avant', 'la', 'place', 'du', 'Tribunal'], ['INTJ', 'PRON', 'PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'INTJ', 'ADJ', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN']]\n"
     ]
    }
   ],
   "source": [
    "print(train_set_spoken[0])\n",
    "print(test_set_spoken[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_3_gram(one_3_gram,dict_of_3_gram,N,V,d):\n",
    "    if one_3_gram in dict_of_3_gram:\n",
    "        return (dict_of_3_gram[one_3_gram]+1)/(N + V*(d-2))\n",
    "    else:\n",
    "        return 1/(N +V*(d-2))\n",
    "\n",
    "#?????????????????????????????????????????????#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_3_gram(train_set,test_set):\n",
    "    \n",
    "    train_sentence, train_label = seperate_sentence_label(train_set)\n",
    "    test_sentence, test_label = seperate_sentence_label(test_set)\n",
    "    \n",
    "    train_words = []\n",
    "    test_words = []\n",
    "    \n",
    "    for sentence in train_sentence:\n",
    "        for word in sentence:\n",
    "            train_words.append(word)\n",
    "    \n",
    "    for sentence in test_sentence:\n",
    "        for word in sentence:    \n",
    "            test_words.append(word)\n",
    "            \n",
    "    \n",
    "    d_train = len(train_words)\n",
    "    d_test = len(test_words)\n",
    "    \n",
    "    all_3_gram = []\n",
    "    train_3_gram = []\n",
    "    test_3_gram = []\n",
    "    \n",
    "    for sentence in train_sentence:\n",
    "        for i in range(2,len(sentence)):\n",
    "            train_3_gram.append(tuple(sentence[j] for j in range(i-2,i+1)))\n",
    "            \n",
    "    for sentence in test_sentence:\n",
    "        for i in range(2,len(sentence)):\n",
    "            test_3_gram.append(tuple(sentence[j] for j in range(i-2,i+1)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    all_3_gram = train_3_gram + test_3_gram\n",
    "    \n",
    "    N = len(all_3_gram)\n",
    "    \n",
    "    num_of_3_gram_train = defaultdict(float)\n",
    "    num_of_3_gram_test = defaultdict(float)\n",
    "    \n",
    "    for one_3_gram in train_3_gram:\n",
    "        num_of_3_gram_train[one_3_gram] += 1.\n",
    "\n",
    "\n",
    "    for one_3_gram in test_3_gram:\n",
    "        num_of_3_gram_test[one_3_gram] += 1.\n",
    "    \n",
    "    V = set(all_3_gram)\n",
    "    \n",
    "    KL = 0.\n",
    "    prob = 0.\n",
    "    prob1 = 0.\n",
    "    \n",
    "    for one_3_gram in V:\n",
    "        #print(prob_3_gram(one_3_gram,num_of_3_gram_test,N,len(V),d_train),prob_3_gram(one_3_gram,num_of_3_gram_train,N,len(V),d_train),num_of_3_gram_test[one_3_gram],num_of_3_gram_train[one_3_gram])\n",
    "        \n",
    "        KL += prob_3_gram(one_3_gram,num_of_3_gram_test,N,len(V),d_test)*np.log(prob_3_gram(one_3_gram,num_of_3_gram_test,N,len(V),d_test)/prob_3_gram(one_3_gram,num_of_3_gram_train,N,len(V),d_train))\n",
    "        prob += prob_3_gram(one_3_gram,num_of_3_gram_test,N,len(V),d_test)\n",
    "        prob1 += prob_3_gram(one_3_gram,num_of_3_gram_train,N,len(V),d_train)\n",
    "    \n",
    "    print(prob)\n",
    "    print(prob1)\n",
    "    print('N = ',N)\n",
    "    print('V = ',len(V))\n",
    "    print('d_test = ',d_test)\n",
    "    print('d_train = ',d_train)\n",
    "    print('test3gram = ',len(test_3_gram))\n",
    "    print('train3gram = ',len(train_3_gram),'\\n')\n",
    "\n",
    "    return KL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6056529107031707e-05\n",
      "4.9997450963283725e-06\n",
      "N =  482731\n",
      "V =  340809\n",
      "d_test =  75073\n",
      "d_train =  442228\n",
      "test3gram =  70003\n",
      "train3gram =  412728 \n",
      "\n",
      "0.00010605357501940126\n",
      "6.3011868030780986e-06\n",
      "N =  325019\n",
      "V =  269267\n",
      "d_test =  9742\n",
      "d_train =  345009\n",
      "test3gram =  8910\n",
      "train3gram =  316109 \n",
      "\n",
      "0.000443854264132652\n",
      "8.99190501325038e-05\n",
      "N =  24013\n",
      "V =  19794\n",
      "d_test =  2515\n",
      "d_train =  23324\n",
      "test3gram =  2295\n",
      "train3gram =  21718 \n",
      "\n",
      "6.535270664616352e-05\n",
      "6.716387760457887e-05\n",
      "N =  43856\n",
      "V =  38339\n",
      "d_test =  24138\n",
      "d_train =  23324\n",
      "test3gram =  22138\n",
      "train3gram =  21718 \n",
      "\n",
      "0.0001232109322786503\n",
      "4.0918421183711724e-05\n",
      "N =  53614\n",
      "V =  44235\n",
      "d_test =  9740\n",
      "d_train =  49173\n",
      "test3gram =  8846\n",
      "train3gram =  44768 \n",
      "\n",
      "0.00014715202995740117\n",
      "0.00011362363664341208\n",
      "N =  21205\n",
      "V =  18098\n",
      "d_test =  10010\n",
      "d_train =  14952\n",
      "test3gram =  8558\n",
      "train3gram =  12647 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "KL_ftb = count_3_gram(train_set_ftb,test_set_ftb)\n",
    "KL_gsd = count_3_gram(train_set_gsd,test_set_gsd)\n",
    "KL_partut = count_3_gram(train_set_partut,test_set_partut)\n",
    "KL_pud = count_3_gram(train_set_pud,test_set_pud)\n",
    "KL_sequoia = count_3_gram(train_set_sequoia,test_set_sequoia)\n",
    "KL_spoken = count_3_gram(train_set_spoken,test_set_spoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.178262556907098e-05\n",
      "0.0003074451277410537\n",
      "0.0007738787470126379\n",
      "1.59910416099936e-05\n",
      "0.00015718860951735862\n",
      "7.696595007138148e-05\n"
     ]
    }
   ],
   "source": [
    "print(KL_ftb)\n",
    "print(KL_gsd)\n",
    "print(KL_partut)\n",
    "print(KL_pud)\n",
    "print(KL_sequoia)\n",
    "print(KL_spoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence_ftb,_ = seperate_sentence_label(train_set_ftb)\n",
    "train_sentence_gsd,_ = seperate_sentence_label(train_set_gsd)\n",
    "train_sentence_partut,_ = seperate_sentence_label(train_set_partut)\n",
    "train_sentence_pud,_ = seperate_sentence_label(train_set_pud)\n",
    "train_sentence_sequoia,_ = seperate_sentence_label(train_set_sequoia)\n",
    "train_sentence_spoken,_ = seperate_sentence_label(train_set_spoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_save(filename, data):#filename为写入CSV文件的路径，data为要写入数据列表.\n",
    "    file = open(filename,'a')\n",
    "    for i in range(len(data)):\n",
    "        s = str(data[i]).replace('[','').replace(']','')#去除[],这两行按数据不同，可以选择\n",
    "        s = s.replace(\"'\",'').replace(',','') +'\\n'   #去除单引号，逗号，每行末尾追加换行符\n",
    "        file.write(s)\n",
    "    file.close()\n",
    "    print(\"success\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "text_save('./train_sentence/train_set_ftb_sentence.txt',train_sentence_ftb)\n",
    "text_save('./train_sentence/train_set_gsd_sentence.txt',train_sentence_gsd)\n",
    "text_save('./train_sentence/train_set_partut_sentence.txt',train_sentence_partut)\n",
    "text_save('./train_sentence/train_set_pud_sentence.txt',train_sentence_pud)\n",
    "text_save('./train_sentence/train_set_sequoia_sentence.txt',train_sentence_sequoia)\n",
    "text_save('./train_sentence/train_set_spoken_sentence.txt',train_sentence_spoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_ftb,_ = seperate_sentence_label(test_set_ftb)\n",
    "test_sentence_gsd,_ = seperate_sentence_label(test_set_gsd)\n",
    "test_sentence_partut,_ = seperate_sentence_label(test_set_partut)\n",
    "test_sentence_pud,_ = seperate_sentence_label(test_set_pud)\n",
    "test_sentence_sequoia,_ = seperate_sentence_label(test_set_sequoia)\n",
    "test_sentence_spoken,_ = seperate_sentence_label(test_set_spoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model,test_set):\n",
    "    model = kenlm.Model(model)\n",
    "    per = 0.\n",
    "    for i in range(len(test_set)):\n",
    "        s = str(test_set[i]).replace('[','').replace(']','')\n",
    "        s = s.replace(\"'\",'').replace(',','')\n",
    "        per += model.perplexity(s)\n",
    "    print(per/len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676.5533438839657\n",
      "534.6275586087717\n",
      "280.4222414479629\n",
      "670.713085413923\n",
      "437.17513496320294\n",
      "308.37358409346643\n"
     ]
    }
   ],
   "source": [
    "calculate_perplexity('train_set_ftb_sentence.arpa',test_sentence_ftb)\n",
    "calculate_perplexity('train_set_gsd_sentence.arpa',test_sentence_gsd)\n",
    "calculate_perplexity('train_set_partut_sentence.arpa',test_sentence_partut)\n",
    "calculate_perplexity('train_set_pud_sentence.arpa',test_sentence_pud)\n",
    "calculate_perplexity('train_set_sequoia_sentence.arpa',test_sentence_sequoia)\n",
    "calculate_perplexity('train_set_spoken_sentence.arpa',test_sentence_spoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5967.06774561587"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.perplexity(\"Je t'aime \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
