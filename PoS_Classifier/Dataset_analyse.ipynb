{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's begin！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_ftb = json.load(open('./corpus/fr/fr.ftb.train.json', encoding = 'utf-8'))\n",
    "test_set_ftb = json.load(open('./corpus/fr/fr.ftb.test.json', encoding = 'utf-8'))\n",
    "train_set_gsd = json.load(open('./corpus/fr/fr.gsd.train.json', encoding = 'utf-8'))\n",
    "test_set_gsd = json.load(open('./corpus/fr/fr.gsd.test.json', encoding = 'utf-8'))\n",
    "train_set_partut = json.load(open('./corpus/fr/fr.partut.train.json', encoding = 'utf-8'))\n",
    "test_set_partut = json.load(open('./corpus/fr/fr.partut.test.json', encoding = 'utf-8'))\n",
    "train_set_pud = json.load(open('./corpus/fr/fr.pud.train.json', encoding = 'utf-8'))\n",
    "test_set_pud = json.load(open('./corpus/fr/fr.pud.test.json', encoding = 'utf-8'))\n",
    "train_set_sequoia = json.load(open('./corpus/fr/fr.sequoia.train.json', encoding = 'utf-8'))\n",
    "test_set_sequoia = json.load(open('./corpus/fr/fr.sequoia.test.json', encoding = 'utf-8'))\n",
    "train_set_spoken = json.load(open('./corpus/fr/fr.spoken.train.json', encoding = 'utf-8'))\n",
    "test_set_spoken = json.load(open('./corpus/fr/fr.spoken.test.json', encoding = 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_sentence_label(data_set):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for sentence,label in data_set:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return sentences,labels\n",
    "\n",
    "train_sentence, train_label = seperate_sentence_label(train_set)\n",
    "test_sentence, test_label = seperate_sentence_label(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_data_set(train_set,test_set):\n",
    "    \n",
    "    difference_between_sentences = []\n",
    "    difference_between_words = []\n",
    "    \n",
    "    difference_between_sentences.append(len(train_set))\n",
    "    difference_between_sentences.append(len(test_set))\n",
    "    difference_between_sentences.append(round(len(train_set)/len(test_set),2))\n",
    "    \n",
    "    train_sentence, train_label = seperate_sentence_label(train_set)\n",
    "    test_sentence, test_label = seperate_sentence_label(test_set)\n",
    "    \n",
    "    train_words_num = np.sum(list(len(sentence) for sentence in train_sentence))\n",
    "    test_words_num = np.sum(list(len(sentence) for sentence in test_sentence))\n",
    "    \n",
    "    difference_between_words.append(train_words_num)\n",
    "    difference_between_words.append(test_words_num)\n",
    "    difference_between_words.append(round(train_words_num/test_words_num,2))\n",
    "    \n",
    "    print(difference_between_sentences)\n",
    "    print(difference_between_words,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14759, 2541, 5.81]\n",
      "[442228, 75073, 5.89] \n",
      "\n",
      "[14450, 416, 34.74]\n",
      "[345009, 9742, 35.41] \n",
      "\n",
      "[803, 110, 7.3]\n",
      "[23324, 2515, 9.27] \n",
      "\n",
      "[803, 1000, 0.8]\n",
      "[23324, 24138, 0.97] \n",
      "\n",
      "[2231, 456, 4.89]\n",
      "[49173, 9740, 5.05] \n",
      "\n",
      "[1153, 726, 1.59]\n",
      "[14952, 10010, 1.49] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_data_set(train_set_ftb,test_set_ftb)\n",
    "display_data_set(train_set_gsd,test_set_gsd)\n",
    "display_data_set(train_set_partut,test_set_partut)\n",
    "display_data_set(train_set_pud,test_set_pud)\n",
    "display_data_set(train_set_sequoia,test_set_sequoia)\n",
    "display_data_set(train_set_spoken,test_set_spoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OOV_words_percentage(train_set,test_set):\n",
    "    train_sentence, train_label = seperate_sentence_label(train_set)\n",
    "    test_sentence, test_label = seperate_sentence_label(test_set)\n",
    "    \n",
    "    train_words = set()\n",
    "    test_words = set()\n",
    "    OOV_words = set()\n",
    "    \n",
    "    for sentence in train_sentence:\n",
    "        for word in sentence:\n",
    "            train_words.add(word)\n",
    "    \n",
    "    for sentence in test_sentence:\n",
    "        for word in sentence:    \n",
    "            test_words.add(word)\n",
    "            \n",
    "            if word not in train_words:\n",
    "                OOV_words.add(word)\n",
    "            \n",
    "    print(round(len(list(OOV_words))/len(list(test_words)),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21\n",
      "0.18\n",
      "0.28\n",
      "0.73\n",
      "0.29\n",
      "0.61\n"
     ]
    }
   ],
   "source": [
    "OOV_words_percentage(train_set_ftb,test_set_ftb)\n",
    "OOV_words_percentage(train_set_gsd,test_set_gsd)\n",
    "OOV_words_percentage(train_set_partut,test_set_partut)\n",
    "OOV_words_percentage(train_set_pud,test_set_pud)\n",
    "OOV_words_percentage(train_set_sequoia,test_set_sequoia)\n",
    "OOV_words_percentage(train_set_spoken,test_set_spoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['euh', 'bon', 'pour', 'aller', 'du', 'CRDT', 'à', 'la', 'gare', 'euh', 'de', 'Grenoble', 'je', 'euh', 'ben', 'je', 'sors', 'déjà', 'du', 'CRDT'], ['INTJ', 'INTJ', 'ADP', 'VERB', 'ADP', 'PROPN', 'ADP', 'DET', 'NOUN', 'INTJ', 'ADP', 'PROPN', 'PRON', 'INTJ', 'INTJ', 'PRON', 'VERB', 'ADV', 'ADP', 'PROPN']]\n",
      "[['euh', 'il', 'y', 'a', 'une', 'petite', 'bifurcation', 'euh', 'juste', 'avant', 'la', 'place', 'du', 'Tribunal'], ['INTJ', 'PRON', 'PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'INTJ', 'ADJ', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN']]\n"
     ]
    }
   ],
   "source": [
    "print(train_set_spoken[0])\n",
    "print(test_set_spoken[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_3_gram(one_3_gram,dict_of_3_gram,N,V,d):\n",
    "    if one_3_gram in dict_of_3_gram:\n",
    "        return (dict_of_3_gram[one_3_gram]+1)/(N+V*(d-2))\n",
    "    else:\n",
    "        return 1/(N+V*(d-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_3_gram(train_set,test_set):\n",
    "    \n",
    "    train_sentence, train_label = seperate_sentence_label(train_set)\n",
    "    test_sentence, test_label = seperate_sentence_label(test_set)\n",
    "    \n",
    "    train_words = []\n",
    "    test_words = []\n",
    "    \n",
    "    for sentence in train_sentence:\n",
    "        for word in sentence:\n",
    "            train_words.append(word)\n",
    "    \n",
    "    for sentence in test_sentence:\n",
    "        for word in sentence:    \n",
    "            test_words.append(word)\n",
    "            \n",
    "    d_train = len(train_words)\n",
    "    d_test = len(test_words)\n",
    "    \n",
    "    \n",
    "    all_3_gram = []\n",
    "    train_3_gram = []\n",
    "    test_3_gram = []\n",
    "    \n",
    "    for sentence in train_sentence:\n",
    "        for i in range(2,len(sentence)):\n",
    "            train_3_gram.append(tuple(sentence[j] for j in range(i-2,i+1)))\n",
    "            \n",
    "    for sentence in test_sentence:\n",
    "        for i in range(2,len(sentence)):\n",
    "            test_3_gram.append(tuple(sentence[j] for j in range(i-2,i+1)))\n",
    "    \n",
    "    all_3_gram = train_3_gram + test_3_gram\n",
    "    \n",
    "    N = len(all_3_gram)\n",
    "    \n",
    "    num_of_3_gram_train = {}\n",
    "    num_of_3_gram_test = {}\n",
    "    \n",
    "    for one_3_gram in train_3_gram:\n",
    "        if one_3_gram in num_of_3_gram_train:\n",
    "            num_of_3_gram_train[one_3_gram] += 1\n",
    "        else:\n",
    "            num_of_3_gram_train[one_3_gram] = 0\n",
    "\n",
    "    for one_3_gram in test_3_gram:\n",
    "        if one_3_gram in num_of_3_gram_test:\n",
    "            num_of_3_gram_test[one_3_gram] += 1\n",
    "        else:\n",
    "            num_of_3_gram_test[one_3_gram] = 1\n",
    "\n",
    "    V = len(set(all_3_gram))\n",
    "    \n",
    "    KL = 0.\n",
    "    \n",
    "    print(N,V)\n",
    "    for one_3_gram in set(all_3_gram):\n",
    "        KL += prob_3_gram(one_3_gram,num_of_3_gram_test,N,V,d_test)*np.log(prob_3_gram(one_3_gram,num_of_3_gram_test,N,V,d_test)/prob_3_gram(one_3_gram,num_of_3_gram_train,N,V,d_train))\n",
    "    \n",
    "    return KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482731 340809\n",
      "325019 269267\n",
      "24013 19794\n",
      "43856 38339\n",
      "53614 44235\n",
      "21205 18098\n"
     ]
    }
   ],
   "source": [
    "KL_ftb = count_3_gram(train_set_ftb,test_set_ftb)\n",
    "KL_gsd = count_3_gram(train_set_gsd,test_set_gsd)\n",
    "KL_partut = count_3_gram(train_set_partut,test_set_partut)\n",
    "KL_pud = count_3_gram(train_set_pud,test_set_pud)\n",
    "KL_sequoia = count_3_gram(train_set_sequoia,test_set_sequoia)\n",
    "KL_spoken = count_3_gram(train_set_spoken,test_set_spoken)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9573426239962565e-05\n",
      "0.000374320063804379\n",
      "0.001016440772055542\n",
      "2.9456284889467706e-05\n",
      "0.0002167759026543251\n",
      "0.0001196052677055119\n"
     ]
    }
   ],
   "source": [
    "print(KL_ftb)\n",
    "print(KL_gsd)\n",
    "print(KL_partut)\n",
    "print(KL_pud)\n",
    "print(KL_sequoia)\n",
    "print(KL_spoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
