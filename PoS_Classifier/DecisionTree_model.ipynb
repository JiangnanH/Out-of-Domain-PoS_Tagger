{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree model\n",
    "\n",
    "Features extracted:\n",
    "- Window feature\n",
    "- Suffix feature\n",
    "- Shape feature\n",
    "\n",
    "Best performance: accuracy **92.26%** on corpus $fr.ftb$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data sets and choose one as our corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_ftb = json.load(open('./corpus/fr/fr.ftb.train.json', encoding = 'utf-8'))\n",
    "test_set_ftb = json.load(open('./corpus/fr/fr.ftb.test.json', encoding = 'utf-8'))\n",
    "train_set_gsd = json.load(open('./corpus/fr/fr.gsd.train.json', encoding = 'utf-8'))\n",
    "test_set_gsd = json.load(open('./corpus/fr/fr.gsd.test.json', encoding = 'utf-8'))\n",
    "train_set_partut = json.load(open('./corpus/fr/fr.partut.train.json', encoding = 'utf-8'))\n",
    "test_set_partut = json.load(open('./corpus/fr/fr.partut.test.json', encoding = 'utf-8'))\n",
    "train_set_pud = json.load(open('./corpus/fr/fr.pud.train.json', encoding = 'utf-8'))\n",
    "test_set_pud = json.load(open('./corpus/fr/fr.pud.test.json', encoding = 'utf-8'))\n",
    "train_set_sequoia = json.load(open('./corpus/fr/fr.sequoia.train.json', encoding = 'utf-8'))\n",
    "test_set_sequoia = json.load(open('./corpus/fr/fr.sequoia.test.json', encoding = 'utf-8'))\n",
    "train_set_spoken = json.load(open('./corpus/fr/fr.spoken.train.json', encoding = 'utf-8'))\n",
    "test_set_spoken = json.load(open('./corpus/fr/fr.spoken.test.json', encoding = 'utf-8'))\n",
    "\n",
    "test_set_foot = json.load(open('./corpus/fr/fr.foot.test.json', encoding = 'utf-8'))\n",
    "test_set_natdis = json.load(open('./corpus/fr/fr.natdis.test.json', encoding = 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set_ftb\n",
    "test_set = test_set_ftb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**words_and_labels**: Seperate the words and the labels from the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_and_labels(data_set):\n",
    "    \n",
    "    words = []\n",
    "    labels = []\n",
    "    for sentence,label in data_set:\n",
    "        for w,l in zip(sentence,label):\n",
    "            words.append(w)\n",
    "            labels.append(l)\n",
    "    \n",
    "    return words,labels\n",
    "\n",
    "train_words,train_label = words_and_labels(train_set)\n",
    "test_words,test_label = words_and_labels(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**suffix_one_hot**: Build a dictionary of suffix for train set, the keys of the dictionary are the suffix in type string and the values are their id number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffix_one_hot(train_words):\n",
    "    \n",
    "    suffix_dict = defaultdict(int)\n",
    "    \n",
    "    for word in train_words:\n",
    "        for k in range(1,len(word)):\n",
    "            suffix_dict[word[k:]] += 1\n",
    "    return suffix_dict\n",
    "\n",
    "suffix_dict = suffix_one_hot(train_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_one_hot**: Build a dictionary of words for train set words_dict { 'word' : id }, the keys of the dictionary are the words in type string and the values are their id number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_hot(train_data):\n",
    "    \n",
    "    words_set = set()\n",
    "    words_dict = defaultdict(int)\n",
    "    for word in train_data:\n",
    "        words_set.add(word)\n",
    "    words_set = list(words_set)\n",
    "    \n",
    "    for id,word in enumerate(words_set):\n",
    "        words_dict[word] = id\n",
    "    \n",
    "    return words_dict\n",
    "    \n",
    "train_words_dict = train_one_hot(train_words)\n",
    "train_labels_dict = train_one_hot(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test_one_hot**: Build a dictionary of words for test set, the same id numbers from the dictionary created with the train set are used to represent the word in test set. The words which only appear in test set (the OOV words) are also marked -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_hot(test_data,train_dict):\n",
    "    words_dict = defaultdict(int)\n",
    "    for word in test_data:\n",
    "        if word in train_dict:\n",
    "            words_dict[word] = train_dict[word]\n",
    "        else:\n",
    "            words_dict[word] = -1\n",
    "    return words_dict\n",
    "\n",
    "test_words_dict = test_one_hot(test_words,train_words_dict)\n",
    "test_labels_dict = test_one_hot(test_label,train_labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**feature_window**: Return a list of the window features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_window(i, sentence, words_dict, l=2):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    word = words_dict[sentence[i]]\n",
    "    res.append(word)\n",
    "    \n",
    "    for k in range(1,l+1):\n",
    "        \n",
    "        res.append(words_dict[sentence[i-k]] if i-k>=0 else -1)\n",
    "        res.append(words_dict[sentence[i+k]] if i+k<len(sentence) else -1)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**feature_suffix**: Return a list of the suffix features. The number of suffix is fix to 10, the words which doesn't have enough suffix will own some -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_suffix(i,sentence):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for k in range(1,10):\n",
    "        if sentence[i][-k:] in suffix_dict:\n",
    "            res.append(suffix_dict[sentence[i][-k:]])\n",
    "        else:\n",
    "            res.append(-1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**feature_shape**: Return a list of the shape features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_shape(i,sentence):\n",
    "    \n",
    "    word = sentence[i]\n",
    "\n",
    "    def has_digit(s):\n",
    "\n",
    "        return any(c.isdigit() for c in s)\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    res.append(1 if word.istitle() else 0)\n",
    "    \n",
    "    res.append(1 if word.isupper() else 0)\n",
    "    \n",
    "    res.append(1 if has_digit(word) else 0)\n",
    "\n",
    "    res.append(1 if '-' in word else 0)\n",
    "    \n",
    "    res.append(1 if '_' in word else 0)\n",
    "    \n",
    "    res.append(1 if not word.isalnum() else 0)\n",
    "    \n",
    "    res.append(1 if len(word) > 3 else 0)\n",
    "\n",
    "    res.append(1 if '\\'' in word else 0)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**collect_features_and_labels**: The 'main' function for extracting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_features_and_labels(data_set,words_dict,labels_dict):\n",
    "    \n",
    "    data = []\n",
    "    label = []\n",
    "    \n",
    "    for sentence,labels in data_set:\n",
    "        \n",
    "        for i in range(len(sentence)):\n",
    "            \n",
    "            data_of_word = []\n",
    "\n",
    "            data_of_word += feature_window(i, sentence, words_dict)\n",
    "            data_of_word += feature_suffix(i, sentence)\n",
    "            data_of_word += feature_shape(i,sentence)\n",
    "\n",
    "            data.append(np.array(data_of_word))\n",
    "        \n",
    "            label.append(labels_dict[labels[i]])\n",
    "            \n",
    "    return data,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**oov_features_and_labels**: Construct the data set for the OOV words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oov_features_and_labels(train_data,test_data,test_label):\n",
    "    \n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for word,label in zip(test_data,test_label):\n",
    "        if word[0] == -1:\n",
    "            data.append(word)\n",
    "            labels.append(label)\n",
    "            \n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ambiguous_features_and_labels**: Construct the data set for the ambiguous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ambiguous_features_and_labels(input_data,input_label):\n",
    "    \n",
    "    words = defaultdict(lambda: set())\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for word,label in zip(input_data,input_label):\n",
    "        words[word[0]].add(label)\n",
    "    \n",
    "    for word,label in zip(input_data,input_label):\n",
    "        if(len(words[word[0]]) > 1):\n",
    "            data.append(word)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return data,labels\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all the features from train set and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time =  3.9818809032440186\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "train_data,train_label = collect_features_and_labels(train_set,train_words_dict,train_labels_dict)\n",
    "end = time.time()\n",
    "print('total time = ',end - begin)\n",
    "\n",
    "test_data,test_label = collect_features_and_labels(test_set,test_words_dict,test_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19392 12844  2620 12444 21417 97943 12341  1055   166     7     7    -1\n",
      "    -1    -1     0     0     0     0     0     0     1     0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the **OOV** set and the **Ambuguous** set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_data,oov_label = oov_features_and_labels(train_data,test_data,test_label)\n",
    "ambiguous_data,ambiguous_label = ambiguous_features_and_labels(train_data,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)\n",
    "train_label = np.array(train_label)\n",
    "test_data = np.array(test_data)\n",
    "test_label = np.array(test_label)\n",
    "oov_data = np.array(oov_data)\n",
    "oov_label = np.array(oov_label)\n",
    "ambiguous_data = np.array(ambiguous_data)\n",
    "ambiguous_label = np.array(ambiguous_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442228, 22)\n",
      "(75073, 22)\n",
      "(2529, 22)\n",
      "(212787, 22)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(oov_data.shape)\n",
    "print(ambiguous_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Decision Tree model, training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "tagger = tree.DecisionTreeClassifier()\n",
    "tagger.fit(train_data,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9993668424432646\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_hat = tagger.predict(train_data)\n",
    "print('train accuracy:', accuracy_score(train_hat,train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.9242603865570844\n",
      "amb accuracy: 0.9986841301395292\n",
      "oov accuracy: 0.5599051008303677\n"
     ]
    }
   ],
   "source": [
    "test_hat = tagger.predict(test_data)\n",
    "print('test accuracy:', accuracy_score(test_hat,test_label))\n",
    "amb_hat = tagger.predict(ambiguous_data)\n",
    "print('amb accuracy:', accuracy_score(amb_hat,ambiguous_label))\n",
    "oov_hat = tagger.predict(oov_data)\n",
    "print('oov accuracy:', accuracy_score(oov_hat,oov_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
