{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = json.load(open('./corpus/fr/fr.ftb.train.json', encoding = 'utf-8'))\n",
    "test_set = json.load(open('./corpus/fr/fr.ftb.test.json', encoding = 'utf-8'))\n",
    "\n",
    "train_set = train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_and_labels(data_set):\n",
    "    \n",
    "    words = []\n",
    "    labels = []\n",
    "    for sentence,label in data_set:\n",
    "        for w,l in zip(sentence,label):\n",
    "            words.append(w)\n",
    "            labels.append(l)\n",
    "    \n",
    "    return words,labels\n",
    "\n",
    "train_words,train_label = words_and_labels(train_set)\n",
    "test_words,test_label = words_and_labels(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_and_labels(data_set):\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for sentence,label in data_set:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return sentences,labels\n",
    "\n",
    "train_sentences,_ = sentences_and_labels(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'INTJ': 0, 'PUNCT': 1, 'NUM': 2, 'ADP+PRON': 3, 'ADJ': 4, 'ADV': 5, 'PRON': 6, 'AUX': 7, 'X': 8, 'SCONJ': 9, 'VERB': 10, 'ADP': 11, 'ADP+DET': 12, 'CCONJ': 13, 'PART': 14, 'PROPN': 15, 'DET': 16, 'NOUN': 17})\n"
     ]
    }
   ],
   "source": [
    "def train_one_hot(train_data):\n",
    "    \n",
    "    words_set = set()\n",
    "    words_dict = defaultdict(int)\n",
    "    for word in train_data:\n",
    "        words_set.add(word)\n",
    "    words_set = list(words_set)\n",
    "    \n",
    "    for id,word in enumerate(words_set):\n",
    "        words_dict[word] = id\n",
    "    \n",
    "    return words_dict\n",
    "    \n",
    "train_words_dict = train_one_hot(train_words)\n",
    "train_labels_dict = train_one_hot(train_label)\n",
    "\n",
    "print(train_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlabel_of_words_dict = {}\\nfor key in train_words_dict.keys():\\n    for word,label in zip(train_words,train_label):\\n        if key == word:\\n            label_of_words_dict[train_words_dict[word]] = train_labels_dict[label]\\n            \\nprint(label_of_words_dict[117])\\n'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "label_of_words_dict = {}\n",
    "for key in train_words_dict.keys():\n",
    "    for word,label in zip(train_words,train_label):\n",
    "        if key == word:\n",
    "            label_of_words_dict[train_words_dict[word]] = train_labels_dict[label]\n",
    "            \n",
    "print(label_of_words_dict[117])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'DET': 16, 'NOUN': 17, 'ADP+DET': 12, 'VERB': 10, 'ADP': 11, 'NUM': 2, 'ADJ': 4, 'PUNCT': 1, 'AUX': 7, 'ADV': 5, 'CCONJ': 13, 'PRON': 6, 'SCONJ': 9, 'PROPN': 15, 'PART': 14, 'X': 8, 'ADP+PRON': 3, 'INTJ': 0, 'ADP+ADP': 0})\n"
     ]
    }
   ],
   "source": [
    "def test_one_hot(test_data,train_dict):\n",
    "    words_dict = defaultdict(int)\n",
    "    for word in test_data:\n",
    "        if word in train_dict:\n",
    "            words_dict[word] = train_dict[word]\n",
    "        else:\n",
    "            words_dict[word] = 0\n",
    "    return words_dict\n",
    "\n",
    "test_words_dict = test_one_hot(test_words,train_words_dict)\n",
    "test_labels_dict = test_one_hot(test_label,train_labels_dict)\n",
    "\n",
    "print(test_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8985, 26780), (26780, 21921), (13683, 26780), (26780, 7898)]\n"
     ]
    }
   ],
   "source": [
    "def feature_window(i, sentence,words_dict,l=2):\n",
    "    '''\n",
    "    i : the index of the word in the context\n",
    "    context : the sentence\n",
    "    l : a window of size is 2*l+1\n",
    "    \n",
    "    return : list of features which are tuple (feature_name, value)\n",
    "    '''\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    word = words_dict[sentence[i]]\n",
    "    \n",
    "    for k in range(1,l+1):\n",
    "        \n",
    "        if i-k >= 0:\n",
    "            res.append((words_dict[sentence[i-k]],word))\n",
    "            \n",
    "        if i+k<len(sentence):\n",
    "            res.append((word,words_dict[sentence[i+k]]))\n",
    "        \n",
    "    return res\n",
    "\n",
    "print(feature_window(7,train_sentences[4],train_words_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_suffix(i,sentence):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for k in range(1,len(sentence[i])):\n",
    "        res.append('suffix_'+sentence[i][k:])\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_shape(word):\n",
    "    '''\n",
    "    i : the index of the word in the context\n",
    "    context : the sentence\n",
    "    \n",
    "    return : list of features which are tuple (feature_name, value)\n",
    "    '''\n",
    "    def has_digit(s):\n",
    "        '''\n",
    "        check if a string has digit or nor\n",
    "        '''\n",
    "        return any(c.isdigit() for c in s)\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "       ## different orthographic\n",
    "    # banary feature indicating whether the word starts with a capital letter or not, 1:yes, 0:not\n",
    "    res.append(1 if word.istitle() else 0)\n",
    "    # banary feature indicating whether the word is made of all capital letters or not, 1:yes, 0:not\n",
    "    res.append(1 if word.isupper() else 0)\n",
    "    # banary feature indicating whether the word has a digit or not, 1:yes, 0:not\n",
    "    res.append(1 if has_digit(word) else 0)\n",
    "    # banary feature indicating whether the word has a hyphen or not, 1:yes, 0:not\n",
    "    res.append(1 if '-' in word else 0)\n",
    "    # banary feature indicating whether the word has a low hyphen or not, 1:yes, 0:not\n",
    "    res.append(1 if '_' in word else 0)\n",
    "    # banary feature indicating whether the letters in the word are all alphanumeric or not, 1:yes, 0:not\n",
    "    res.append(1 if not word.isalnum() else 0)\n",
    "    # binary feature indicating whether the length of word is more than 3\n",
    "    res.append(1 if len(word) > 3 else 0)\n",
    "\n",
    "    res.append(1 if '\\'' in word else 0)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_features_and_labels(data_set,words_dict,labels_dict):\n",
    "    \n",
    "    data = []\n",
    "    label = []\n",
    "    \n",
    "    #punct = pick_out_punct(data_set)\n",
    "    \n",
    "    #bigram_left,bigram_right = get_bigram(data_set)\n",
    "    \n",
    "    for sentence,labels in data_set:\n",
    "        \n",
    "        for i in range(len(sentence)):\n",
    "              \n",
    "            data_of_word = []\n",
    "            \n",
    "            data_of_word += feature_window(i, sentence, words_dict)\n",
    "\n",
    "            data += data_of_word\n",
    "            label.append(labels_dict[labels[i]])\n",
    "            \n",
    "    return data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_other_features(words_dict):\n",
    "    \n",
    "    other_features = []\n",
    "    #punct = pick_out_punct(data_set)\n",
    "    \n",
    "    #bigram_left,bigram_right = get_bigram(data_set)\n",
    "    \n",
    "    for word in words_dict.keys():\n",
    "        \n",
    "        other_feature = []\n",
    "\n",
    "        other_feature += feature_shape(str(word))\n",
    "\n",
    "        other_features.append(other_feature)\n",
    "            \n",
    "    return np.array(other_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_labels = collect_features_and_labels(train_set,train_words_dict,train_labels_dict)\n",
    "#test_data,test_labels = collect_features_and_labels(test_set,test_words_dict,test_labels_dict)\n",
    "other_feature = collect_other_features(train_words_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680394\n",
      "442228\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27127\n",
      "(4153, 19757)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_words_dict))\n",
    "#print(train_words_dict)\n",
    "print(train_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4153, 4153, 4153, 19757, 19757, 19757, 4836, 4153, 4836, 4836]\n",
      "[19757, 4836, 19757, 4836, 15401, 4836, 15401, 4836, 12764, 15401]\n",
      "0: loss: 10.348618\n",
      "0: loss: 10.318377\n",
      "0: loss: 10.288097\n",
      "0: loss: 10.257918\n",
      "0: loss: 10.227615\n",
      "0: loss: 10.197393\n",
      "0: loss: 10.16708\n",
      "0: loss: 10.136783\n",
      "0: loss: 10.106353\n",
      "0: loss: 10.076146\n",
      "0: loss: 10.045656\n",
      "0: loss: 10.015247\n",
      "0: loss: 9.98475\n",
      "0: loss: 9.954364\n",
      "0: loss: 9.923791\n",
      "0: loss: 9.893293\n",
      "0: loss: 9.862786\n",
      "0: loss: 9.832128\n",
      "0: loss: 9.801491\n",
      "0: loss: 9.770954\n",
      "0: loss: 9.740245\n",
      "0: loss: 9.709492\n",
      "0: loss: 9.678748\n",
      "0: loss: 9.647994\n",
      "0: loss: 9.617119\n",
      "0: loss: 9.586285\n",
      "0: loss: 9.555525\n",
      "0: loss: 9.524768\n",
      "0: loss: 9.494012\n",
      "0: loss: 9.463584\n",
      "0: loss: 9.433148\n",
      "0: loss: 9.40297\n",
      "0: loss: 9.372975\n",
      "0: loss: 9.343113\n",
      "0: loss: 9.313692\n",
      "0: loss: 9.28453\n",
      "0: loss: 9.255855\n",
      "0: loss: 9.227308\n",
      "0: loss: 9.199173\n",
      "0: loss: 9.17132\n",
      "0: loss: 9.143838\n",
      "0: loss: 9.116605\n",
      "0: loss: 9.089972\n",
      "0: loss: 9.063699\n",
      "0: loss: 9.037311\n",
      "0: loss: 9.011711\n",
      "0: loss: 8.986027\n",
      "0: loss: 8.96064\n",
      "0: loss: 8.935636\n",
      "0: loss: 8.910854\n",
      "0: loss: 8.886239\n",
      "0: loss: 8.862001\n",
      "0: loss: 8.837744\n",
      "0: loss: 8.813639\n",
      "0: loss: 8.789752\n",
      "0: loss: 8.765477\n",
      "0: loss: 8.741999\n",
      "0: loss: 8.718653\n",
      "0: loss: 8.69527\n",
      "0: loss: 8.672329\n",
      "0: loss: 8.64895\n",
      "0: loss: 8.625883\n",
      "0: loss: 8.603056\n",
      "0: loss: 8.580088\n",
      "0: loss: 8.558189\n",
      "0: loss: 8.5361\n",
      "0: loss: 8.51411\n",
      "0: loss: 8.492581\n",
      "0: loss: 8.470527\n",
      "0: loss: 8.448988\n",
      "0: loss: 8.427487\n",
      "0: loss: 8.405914\n",
      "0: loss: 8.385571\n",
      "0: loss: 8.364781\n",
      "0: loss: 8.344364\n",
      "0: loss: 8.324156\n",
      "0: loss: 8.303823\n",
      "0: loss: 8.283917\n",
      "0: loss: 8.263748\n",
      "0: loss: 8.244774\n",
      "0: loss: 8.225511\n",
      "0: loss: 8.206331\n",
      "0: loss: 8.187923\n",
      "0: loss: 8.169168\n",
      "0: loss: 8.150605\n",
      "0: loss: 8.133125\n",
      "0: loss: 8.115018\n",
      "0: loss: 8.0972395\n",
      "0: loss: 8.079437\n",
      "0: loss: 8.061606\n",
      "0: loss: 8.044814\n",
      "0: loss: 8.027869\n",
      "0: loss: 8.011314\n",
      "0: loss: 7.993975\n",
      "0: loss: 7.9770236\n",
      "0: loss: 7.9602757\n",
      "0: loss: 7.943373\n",
      "0: loss: 7.927409\n",
      "0: loss: 7.911199\n",
      "0: loss: 7.8950663\n",
      "0: loss: 7.879092\n",
      "0: loss: 7.8631296\n",
      "0: loss: 7.8474507\n",
      "0: loss: 7.831772\n",
      "0: loss: 7.8165874\n",
      "0: loss: 7.801162\n",
      "0: loss: 7.7856913\n",
      "0: loss: 7.77085\n",
      "0: loss: 7.7556047\n",
      "0: loss: 7.7407265\n",
      "0: loss: 7.7260475\n",
      "0: loss: 7.710595\n",
      "0: loss: 7.69618\n",
      "0: loss: 7.681172\n",
      "0: loss: 7.6667624\n",
      "0: loss: 7.652466\n",
      "0: loss: 7.637736\n",
      "0: loss: 7.623644\n",
      "0: loss: 7.608939\n",
      "0: loss: 7.5950046\n",
      "0: loss: 7.5811157\n",
      "0: loss: 7.567302\n",
      "0: loss: 7.553525\n",
      "0: loss: 7.5396132\n",
      "0: loss: 7.526131\n",
      "0: loss: 7.5130615\n",
      "0: loss: 7.4996843\n",
      "0: loss: 7.486829\n",
      "0: loss: 7.473185\n",
      "0: loss: 7.45936\n",
      "0: loss: 7.4465804\n",
      "0: loss: 7.4330125\n",
      "0: loss: 7.4201946\n",
      "0: loss: 7.407036\n",
      "0: loss: 7.39425\n",
      "0: loss: 7.3807654\n",
      "0: loss: 7.3673954\n",
      "0: loss: 7.35418\n",
      "0: loss: 7.3410654\n",
      "0: loss: 7.3279705\n",
      "0: loss: 7.31542\n",
      "0: loss: 7.3030057\n",
      "0: loss: 7.2900076\n",
      "0: loss: 7.277754\n",
      "0: loss: 7.264939\n",
      "0: loss: 7.2522655\n",
      "0: loss: 7.2403374\n",
      "0: loss: 7.229087\n",
      "0: loss: 7.217218\n",
      "0: loss: 7.205693\n",
      "0: loss: 7.1936727\n",
      "0: loss: 7.1821237\n",
      "0: loss: 7.1707797\n",
      "0: loss: 7.1599727\n",
      "0: loss: 7.1487627\n",
      "0: loss: 7.1375704\n",
      "0: loss: 7.1264\n",
      "0: loss: 7.115469\n",
      "0: loss: 7.104366\n",
      "0: loss: 7.0934596\n",
      "0: loss: 7.08308\n",
      "0: loss: 7.0723357\n",
      "0: loss: 7.0617805\n",
      "0: loss: 7.0510907\n",
      "0: loss: 7.0406623\n",
      "0: loss: 7.0298533\n",
      "0: loss: 7.0192804\n",
      "0: loss: 7.008046\n",
      "1: loss: 7.012661\n",
      "1: loss: 7.0021343\n",
      "1: loss: 6.9908876\n",
      "1: loss: 6.9804735\n",
      "1: loss: 6.969198\n",
      "1: loss: 6.9585576\n",
      "1: loss: 6.9471984\n",
      "1: loss: 6.9366326\n",
      "1: loss: 6.925472\n",
      "1: loss: 6.9148765\n",
      "1: loss: 6.9037704\n",
      "1: loss: 6.8927875\n",
      "1: loss: 6.8817368\n",
      "1: loss: 6.8715634\n",
      "1: loss: 6.8607173\n",
      "1: loss: 6.8507338\n",
      "1: loss: 6.840451\n",
      "1: loss: 6.8304467\n",
      "1: loss: 6.8205967\n",
      "1: loss: 6.810961\n",
      "1: loss: 6.8011\n",
      "1: loss: 6.7912664\n",
      "1: loss: 6.7817264\n",
      "1: loss: 6.7720704\n",
      "1: loss: 6.7624154\n",
      "1: loss: 6.752775\n",
      "1: loss: 6.7432375\n",
      "1: loss: 6.733675\n",
      "1: loss: 6.7240896\n",
      "1: loss: 6.7154517\n",
      "1: loss: 6.7060046\n",
      "1: loss: 6.6971273\n",
      "1: loss: 6.6877704\n",
      "1: loss: 6.678368\n",
      "1: loss: 6.6695447\n",
      "1: loss: 6.660257\n",
      "1: loss: 6.65185\n",
      "1: loss: 6.6426954\n",
      "1: loss: 6.633547\n",
      "1: loss: 6.6240015\n",
      "1: loss: 6.6148577\n",
      "1: loss: 6.605228\n",
      "1: loss: 6.596507\n",
      "1: loss: 6.587637\n",
      "1: loss: 6.5781507\n",
      "1: loss: 6.569495\n",
      "1: loss: 6.559968\n",
      "1: loss: 6.5507045\n",
      "1: loss: 6.5420523\n",
      "1: loss: 6.5338655\n",
      "1: loss: 6.5253005\n",
      "1: loss: 6.5173697\n",
      "1: loss: 6.5087132\n",
      "1: loss: 6.500319\n",
      "1: loss: 6.4918947\n",
      "1: loss: 6.4829135\n",
      "1: loss: 6.4750896\n",
      "1: loss: 6.466562\n",
      "1: loss: 6.458347\n",
      "1: loss: 6.4504204\n",
      "1: loss: 6.4415145\n",
      "1: loss: 6.432552\n",
      "1: loss: 6.424151\n",
      "1: loss: 6.4151125\n",
      "1: loss: 6.4068904\n",
      "1: loss: 6.398436\n",
      "1: loss: 6.389982\n",
      "1: loss: 6.38229\n",
      "1: loss: 6.373264\n",
      "1: loss: 6.364999\n",
      "1: loss: 6.356646\n",
      "1: loss: 6.3477774\n",
      "1: loss: 6.340278\n",
      "1: loss: 6.332047\n",
      "1: loss: 6.3240623\n",
      "1: loss: 6.316218\n",
      "1: loss: 6.307775\n",
      "1: loss: 6.2999344\n",
      "1: loss: 6.291477\n",
      "1: loss: 6.2842426\n",
      "1: loss: 6.2765346\n",
      "1: loss: 6.268548\n",
      "1: loss: 6.2613115\n",
      "1: loss: 6.253365\n",
      "1: loss: 6.2456346\n",
      "1: loss: 6.2386\n",
      "1: loss: 6.2307854\n",
      "1: loss: 6.223181\n",
      "1: loss: 6.2153935\n",
      "1: loss: 6.207218\n",
      "1: loss: 6.200338\n",
      "1: loss: 6.193018\n",
      "1: loss: 6.1855464\n",
      "1: loss: 6.1775665\n",
      "1: loss: 6.1698093\n",
      "1: loss: 6.161915\n",
      "1: loss: 6.1540036\n",
      "1: loss: 6.147141\n",
      "1: loss: 6.1396017\n",
      "1: loss: 6.1319785\n",
      "1: loss: 6.1244707\n",
      "1: loss: 6.1165576\n",
      "1: loss: 6.1090064\n",
      "1: loss: 6.101351\n",
      "1: loss: 6.094194\n",
      "1: loss: 6.0867047\n",
      "1: loss: 6.07885\n",
      "1: loss: 6.07188\n",
      "1: loss: 6.0639944\n",
      "1: loss: 6.056753\n",
      "1: loss: 6.0495048\n",
      "1: loss: 6.0414834\n",
      "1: loss: 6.0347834\n",
      "1: loss: 6.0268726\n",
      "1: loss: 6.0197678\n",
      "1: loss: 6.0124993\n",
      "1: loss: 6.004605\n",
      "1: loss: 5.9974394\n",
      "1: loss: 5.9896736\n",
      "1: loss: 5.982713\n",
      "1: loss: 5.975529\n",
      "1: loss: 5.968475\n",
      "1: loss: 5.9612594\n",
      "1: loss: 5.9536963\n",
      "1: loss: 5.9468236\n",
      "1: loss: 5.9399977\n",
      "1: loss: 5.9330482\n",
      "1: loss: 5.926375\n",
      "1: loss: 5.9188213\n",
      "1: loss: 5.911242\n",
      "1: loss: 5.904441\n",
      "1: loss: 5.897039\n",
      "1: loss: 5.890235\n",
      "1: loss: 5.8830576\n",
      "1: loss: 5.8759775\n",
      "1: loss: 5.8684983\n",
      "1: loss: 5.860934\n",
      "1: loss: 5.8534937\n",
      "1: loss: 5.846236\n",
      "1: loss: 5.838818\n",
      "1: loss: 5.831819\n",
      "1: loss: 5.824848\n",
      "1: loss: 5.8173513\n",
      "1: loss: 5.810539\n",
      "1: loss: 5.803106\n",
      "1: loss: 5.795784\n",
      "1: loss: 5.789135\n",
      "1: loss: 5.783184\n",
      "1: loss: 5.7764354\n",
      "1: loss: 5.770288\n",
      "1: loss: 5.763312\n",
      "1: loss: 5.7568316\n",
      "1: loss: 5.75049\n",
      "1: loss: 5.7447333\n",
      "1: loss: 5.7385015\n",
      "1: loss: 5.732174\n",
      "1: loss: 5.7260256\n",
      "1: loss: 5.7200646\n",
      "1: loss: 5.7137327\n",
      "1: loss: 5.7077107\n",
      "1: loss: 5.702033\n",
      "1: loss: 5.6959863\n",
      "1: loss: 5.6901965\n",
      "1: loss: 5.6841836\n",
      "1: loss: 5.678427\n",
      "1: loss: 5.6719656\n",
      "1: loss: 5.6658826\n",
      "1: loss: 5.6586947\n",
      "2: loss: 5.6641297\n",
      "2: loss: 5.657759\n",
      "2: loss: 5.6502986\n",
      "2: loss: 5.6439767\n",
      "2: loss: 5.63675\n",
      "2: loss: 5.6300154\n",
      "2: loss: 5.62256\n",
      "2: loss: 5.615825\n",
      "2: loss: 5.608527\n",
      "2: loss: 5.601588\n",
      "2: loss: 5.5942507\n",
      "2: loss: 5.587096\n",
      "2: loss: 5.5797787\n",
      "2: loss: 5.5731974\n",
      "2: loss: 5.5660844\n",
      "2: loss: 5.5597663\n",
      "2: loss: 5.5529203\n",
      "2: loss: 5.5465283\n",
      "2: loss: 5.5402474\n",
      "2: loss: 5.53423\n",
      "2: loss: 5.5277405\n",
      "2: loss: 5.521315\n",
      "2: loss: 5.5153017\n",
      "2: loss: 5.5090427\n",
      "2: loss: 5.5028205\n",
      "2: loss: 5.49651\n",
      "2: loss: 5.4902983\n",
      "2: loss: 5.4840045\n",
      "2: loss: 5.477672\n",
      "2: loss: 5.4722958\n",
      "2: loss: 5.466046\n",
      "2: loss: 5.4604893\n",
      "2: loss: 5.454284\n",
      "2: loss: 5.4480195\n",
      "2: loss: 5.4423847\n",
      "2: loss: 5.436168\n",
      "2: loss: 5.430843\n",
      "2: loss: 5.4248343\n",
      "2: loss: 5.418746\n",
      "2: loss: 5.4124413\n",
      "2: loss: 5.406279\n",
      "2: loss: 5.399631\n",
      "2: loss: 5.394035\n",
      "2: loss: 5.3881264\n",
      "2: loss: 5.381666\n",
      "2: loss: 5.3758945\n",
      "2: loss: 5.3692904\n",
      "2: loss: 5.3629837\n",
      "2: loss: 5.3571367\n",
      "2: loss: 5.351725\n",
      "2: loss: 5.345996\n",
      "2: loss: 5.3408227\n",
      "2: loss: 5.334856\n",
      "2: loss: 5.3293037\n",
      "2: loss: 5.3236804\n",
      "2: loss: 5.3177433\n",
      "2: loss: 5.3126388\n",
      "2: loss: 5.3067775\n",
      "2: loss: 5.3014655\n",
      "2: loss: 5.296193\n",
      "2: loss: 5.290023\n",
      "2: loss: 5.2837486\n",
      "2: loss: 5.2780356\n",
      "2: loss: 5.2716455\n",
      "2: loss: 5.266043\n",
      "2: loss: 5.2601852\n",
      "2: loss: 5.2542505\n",
      "2: loss: 5.2491527\n",
      "2: loss: 5.2427077\n",
      "2: loss: 5.236976\n",
      "2: loss: 5.2312307\n",
      "2: loss: 5.224928\n",
      "2: loss: 5.2198567\n",
      "2: loss: 5.2142043\n",
      "2: loss: 5.208683\n",
      "2: loss: 5.2034016\n",
      "2: loss: 5.1974173\n",
      "2: loss: 5.192004\n",
      "2: loss: 5.186153\n",
      "2: loss: 5.1812534\n",
      "2: loss: 5.176015\n",
      "2: loss: 5.1704283\n",
      "2: loss: 5.1654854\n",
      "2: loss: 5.1599216\n",
      "2: loss: 5.1545444\n",
      "2: loss: 5.1498084\n",
      "2: loss: 5.144314\n",
      "2: loss: 5.1390123\n",
      "2: loss: 5.1335545\n",
      "2: loss: 5.127773\n",
      "2: loss: 5.123146\n",
      "2: loss: 5.1178985\n",
      "2: loss: 5.112655\n",
      "2: loss: 5.107117\n",
      "2: loss: 5.1015677\n",
      "2: loss: 5.0960836\n",
      "2: loss: 5.0905538\n",
      "2: loss: 5.0858045\n",
      "2: loss: 5.080493\n",
      "2: loss: 5.0752516\n",
      "2: loss: 5.0699944\n",
      "2: loss: 5.0642133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: loss: 5.059038\n",
      "2: loss: 5.053519\n",
      "2: loss: 5.048362\n",
      "2: loss: 5.043108\n",
      "2: loss: 5.0374236\n",
      "2: loss: 5.0326514\n",
      "2: loss: 5.0269866\n",
      "2: loss: 5.0218697\n",
      "2: loss: 5.0167165\n",
      "2: loss: 5.0109315\n",
      "2: loss: 5.006338\n",
      "2: loss: 5.0006633\n",
      "2: loss: 4.995759\n",
      "2: loss: 4.990439\n",
      "2: loss: 4.9846706\n",
      "2: loss: 4.9794564\n",
      "2: loss: 4.973837\n",
      "2: loss: 4.969036\n",
      "2: loss: 4.9638352\n",
      "2: loss: 4.958871\n",
      "2: loss: 4.953597\n",
      "2: loss: 4.9480724\n",
      "2: loss: 4.9433055\n",
      "2: loss: 4.938396\n",
      "2: loss: 4.933521\n",
      "2: loss: 4.928733\n",
      "2: loss: 4.923167\n",
      "2: loss: 4.9176393\n",
      "2: loss: 4.9127493\n",
      "2: loss: 4.9074316\n",
      "2: loss: 4.902587\n",
      "2: loss: 4.8973517\n",
      "2: loss: 4.8922186\n",
      "2: loss: 4.8868876\n",
      "2: loss: 4.8813267\n",
      "2: loss: 4.875934\n",
      "2: loss: 4.8708053\n",
      "2: loss: 4.8654513\n",
      "2: loss: 4.860354\n",
      "2: loss: 4.855289\n",
      "2: loss: 4.8498406\n",
      "2: loss: 4.84492\n",
      "2: loss: 4.8394947\n",
      "2: loss: 4.8341517\n",
      "2: loss: 4.8293834\n",
      "2: loss: 4.825291\n",
      "2: loss: 4.8204055\n",
      "2: loss: 4.8162394\n",
      "2: loss: 4.8110876\n",
      "2: loss: 4.806621\n",
      "2: loss: 4.80226\n",
      "2: loss: 4.7983584\n",
      "2: loss: 4.794093\n",
      "2: loss: 4.7897186\n",
      "2: loss: 4.785467\n",
      "2: loss: 4.7814546\n",
      "2: loss: 4.7771006\n",
      "2: loss: 4.772926\n",
      "2: loss: 4.7690625\n",
      "2: loss: 4.764946\n",
      "2: loss: 4.7609873\n",
      "2: loss: 4.7568374\n",
      "2: loss: 4.7529836\n",
      "2: loss: 4.7483687\n",
      "2: loss: 4.7440944\n",
      "2: loss: 4.7387404\n",
      "3: loss: 4.7447968\n",
      "3: loss: 4.7401743\n",
      "3: loss: 4.7344446\n",
      "3: loss: 4.7298684\n",
      "3: loss: 4.724338\n",
      "3: loss: 4.7192345\n",
      "3: loss: 4.713454\n",
      "3: loss: 4.7083244\n",
      "3: loss: 4.7027326\n",
      "3: loss: 4.697351\n",
      "3: loss: 4.691682\n",
      "3: loss: 4.6862564\n",
      "3: loss: 4.6806183\n",
      "3: loss: 4.6755614\n",
      "3: loss: 4.670172\n",
      "3: loss: 4.665459\n",
      "3: loss: 4.660154\n",
      "3: loss: 4.6553884\n",
      "3: loss: 4.650706\n",
      "3: loss: 4.646377\n",
      "3: loss: 4.641392\n",
      "3: loss: 4.636561\n",
      "3: loss: 4.6321936\n",
      "3: loss: 4.6275544\n",
      "3: loss: 4.6230006\n",
      "3: loss: 4.6183057\n",
      "3: loss: 4.613703\n",
      "3: loss: 4.609006\n",
      "3: loss: 4.6042795\n",
      "3: loss: 4.6004972\n",
      "3: loss: 4.5958524\n",
      "3: loss: 4.5919304\n",
      "3: loss: 4.5873046\n",
      "3: loss: 4.5826263\n",
      "3: loss: 4.578565\n",
      "3: loss: 4.5738983\n",
      "3: loss: 4.5701547\n",
      "3: loss: 4.565776\n",
      "3: loss: 4.561282\n",
      "3: loss: 4.556782\n",
      "3: loss: 4.552171\n",
      "3: loss: 4.5471406\n",
      "3: loss: 4.5432506\n",
      "3: loss: 4.538957\n",
      "3: loss: 4.5341763\n",
      "3: loss: 4.5299535\n",
      "3: loss: 4.5250015\n",
      "3: loss: 4.5203557\n",
      "3: loss: 4.5160046\n",
      "3: loss: 4.5120773\n",
      "3: loss: 4.5079384\n",
      "3: loss: 4.504297\n",
      "3: loss: 4.499868\n",
      "3: loss: 4.495939\n",
      "3: loss: 4.491927\n",
      "3: loss: 4.487589\n",
      "3: loss: 4.4839635\n",
      "3: loss: 4.479664\n",
      "3: loss: 4.4758577\n",
      "3: loss: 4.4720826\n",
      "3: loss: 4.4675865\n",
      "3: loss: 4.462915\n",
      "3: loss: 4.4587846\n",
      "3: loss: 4.4539967\n",
      "3: loss: 4.449964\n",
      "3: loss: 4.4456677\n",
      "3: loss: 4.4412365\n",
      "3: loss: 4.4377065\n",
      "3: loss: 4.4328413\n",
      "3: loss: 4.428664\n",
      "3: loss: 4.4245496\n",
      "3: loss: 4.419867\n",
      "3: loss: 4.4162717\n",
      "3: loss: 4.4122095\n",
      "3: loss: 4.4082303\n",
      "3: loss: 4.4045534\n",
      "3: loss: 4.400139\n",
      "3: loss: 4.396232\n",
      "3: loss: 4.392041\n",
      "3: loss: 4.388597\n",
      "3: loss: 4.3849\n",
      "3: loss: 4.380879\n",
      "3: loss: 4.3773866\n",
      "3: loss: 4.3733816\n",
      "3: loss: 4.3695173\n",
      "3: loss: 4.3662686\n",
      "3: loss: 4.36228\n",
      "3: loss: 4.3584948\n",
      "3: loss: 4.3545966\n",
      "3: loss: 4.3504367\n",
      "3: loss: 4.3473\n",
      "3: loss: 4.343418\n",
      "3: loss: 4.339677\n",
      "3: loss: 4.3357277\n",
      "3: loss: 4.331655\n",
      "3: loss: 4.3278937\n",
      "3: loss: 4.3239822\n",
      "3: loss: 4.320627\n",
      "3: loss: 4.3168945\n",
      "3: loss: 4.3133483\n",
      "3: loss: 4.309551\n",
      "3: loss: 4.305354\n",
      "3: loss: 4.3018327\n",
      "3: loss: 4.2978215\n",
      "3: loss: 4.294143\n",
      "3: loss: 4.2903695\n",
      "3: loss: 4.286255\n",
      "3: loss: 4.282972\n",
      "3: loss: 4.2788763\n",
      "3: loss: 4.275223\n",
      "3: loss: 4.2715564\n",
      "3: loss: 4.2673345\n",
      "3: loss: 4.264121\n",
      "3: loss: 4.26\n",
      "3: loss: 4.256617\n",
      "3: loss: 4.252642\n",
      "3: loss: 4.248386\n",
      "3: loss: 4.2447014\n",
      "3: loss: 4.240611\n",
      "3: loss: 4.2373924\n",
      "3: loss: 4.2337\n",
      "3: loss: 4.230243\n",
      "3: loss: 4.226357\n",
      "3: loss: 4.222331\n",
      "3: loss: 4.219099\n",
      "3: loss: 4.2155786\n",
      "3: loss: 4.212228\n",
      "3: loss: 4.208802\n",
      "3: loss: 4.2047124\n",
      "3: loss: 4.200685\n",
      "3: loss: 4.197197\n",
      "3: loss: 4.193339\n",
      "3: loss: 4.189912\n",
      "3: loss: 4.1860847\n",
      "3: loss: 4.1823363\n",
      "3: loss: 4.1785607\n",
      "3: loss: 4.174484\n",
      "3: loss: 4.170612\n",
      "3: loss: 4.1670365\n",
      "3: loss: 4.1632338\n",
      "3: loss: 4.1595306\n",
      "3: loss: 4.1558566\n",
      "3: loss: 4.1519537\n",
      "3: loss: 4.1484323\n",
      "3: loss: 4.1445127\n",
      "3: loss: 4.1406264\n",
      "3: loss: 4.1372523\n",
      "3: loss: 4.1345263\n",
      "3: loss: 4.1310134\n",
      "3: loss: 4.128267\n",
      "3: loss: 4.124477\n",
      "3: loss: 4.121568\n",
      "3: loss: 4.1186957\n",
      "3: loss: 4.116183\n",
      "3: loss: 4.1134124\n",
      "3: loss: 4.1105056\n",
      "3: loss: 4.1076374\n",
      "3: loss: 4.105039\n",
      "3: loss: 4.1021876\n",
      "3: loss: 4.0993733\n",
      "3: loss: 4.0968604\n",
      "3: loss: 4.094176\n",
      "3: loss: 4.0915728\n",
      "3: loss: 4.088783\n",
      "3: loss: 4.0863304\n",
      "3: loss: 4.0830965\n",
      "3: loss: 4.0801578\n",
      "3: loss: 4.0762615\n",
      "4: loss: 4.08385\n",
      "4: loss: 4.080481\n",
      "4: loss: 4.076061\n",
      "4: loss: 4.0727396\n",
      "4: loss: 4.0684805\n",
      "4: loss: 4.0645623\n",
      "4: loss: 4.0600257\n",
      "4: loss: 4.0560803\n",
      "4: loss: 4.051776\n",
      "4: loss: 4.047593\n",
      "4: loss: 4.043177\n",
      "4: loss: 4.0390635\n",
      "4: loss: 4.034663\n",
      "4: loss: 4.0307903\n",
      "4: loss: 4.0267057\n",
      "4: loss: 4.023191\n",
      "4: loss: 4.019073\n",
      "4: loss: 4.0155015\n",
      "4: loss: 4.012016\n",
      "4: loss: 4.0089173\n",
      "4: loss: 4.0051093\n",
      "4: loss: 4.001462\n",
      "4: loss: 3.9983032\n",
      "4: loss: 3.9948695\n",
      "4: loss: 3.99154\n",
      "4: loss: 3.9880407\n",
      "4: loss: 3.984643\n",
      "4: loss: 3.981141\n",
      "4: loss: 3.9776075\n",
      "4: loss: 3.975032\n",
      "4: loss: 3.9715843\n",
      "4: loss: 3.9688704\n",
      "4: loss: 3.9654243\n",
      "4: loss: 3.9619422\n",
      "4: loss: 3.959043\n",
      "4: loss: 3.9555411\n",
      "4: loss: 3.9530015\n",
      "4: loss: 3.9498289\n",
      "4: loss: 3.9465382\n",
      "4: loss: 3.943417\n",
      "4: loss: 3.9399676\n",
      "4: loss: 3.9361703\n",
      "4: loss: 3.9335625\n",
      "4: loss: 3.9304914\n",
      "4: loss: 3.9269884\n",
      "4: loss: 3.923921\n",
      "4: loss: 3.9202425\n",
      "4: loss: 3.9168673\n",
      "4: loss: 3.9136212\n",
      "4: loss: 3.9108226\n",
      "4: loss: 3.9078958\n",
      "4: loss: 3.9054172\n",
      "4: loss: 3.902175\n",
      "4: loss: 3.8995109\n",
      "4: loss: 3.8967311\n",
      "4: loss: 3.893619\n",
      "4: loss: 3.89109\n",
      "4: loss: 3.8879836\n",
      "4: loss: 3.8853028\n",
      "4: loss: 3.8826149\n",
      "4: loss: 3.8794436\n",
      "4: loss: 3.8759813\n",
      "4: loss: 3.87305\n",
      "4: loss: 3.8695009\n",
      "4: loss: 3.8666563\n",
      "4: loss: 3.8635476\n",
      "4: loss: 3.8602376\n",
      "4: loss: 3.857873\n",
      "4: loss: 3.8542016\n",
      "4: loss: 3.851198\n",
      "4: loss: 3.8483086\n",
      "4: loss: 3.8448625\n",
      "4: loss: 3.8423676\n",
      "4: loss: 3.83945\n",
      "4: loss: 3.83663\n",
      "4: loss: 3.834122\n",
      "4: loss: 3.83089\n",
      "4: loss: 3.8281028\n",
      "4: loss: 3.8251445\n",
      "4: loss: 3.8227804\n",
      "4: loss: 3.8201835\n",
      "4: loss: 3.817332\n",
      "4: loss: 3.8149047\n",
      "4: loss: 3.812068\n",
      "4: loss: 3.809314\n",
      "4: loss: 3.8071704\n",
      "4: loss: 3.804311\n",
      "4: loss: 3.80166\n",
      "4: loss: 3.7989273\n",
      "4: loss: 3.7959843\n",
      "4: loss: 3.7939386\n",
      "4: loss: 3.7910993\n",
      "4: loss: 3.7884746\n",
      "4: loss: 3.7856402\n",
      "4: loss: 3.782664\n",
      "4: loss: 3.780107\n",
      "4: loss: 3.777391\n",
      "4: loss: 3.7750921\n",
      "4: loss: 3.7724743\n",
      "4: loss: 3.7701101\n",
      "4: loss: 3.7673657\n",
      "4: loss: 3.7643735\n",
      "4: loss: 3.762\n",
      "4: loss: 3.7591183\n",
      "4: loss: 3.756536\n",
      "4: loss: 3.7538414\n",
      "4: loss: 3.7508926\n",
      "4: loss: 3.7486992\n",
      "4: loss: 3.7457714\n",
      "4: loss: 3.743183\n",
      "4: loss: 3.7406125\n",
      "4: loss: 3.7375164\n",
      "4: loss: 3.7352808\n",
      "4: loss: 3.7322328\n",
      "4: loss: 3.7299266\n",
      "4: loss: 3.72695\n",
      "4: loss: 3.7238054\n",
      "4: loss: 3.7213032\n",
      "4: loss: 3.7183359\n",
      "4: loss: 3.71628\n",
      "4: loss: 3.7136593\n",
      "4: loss: 3.7113023\n",
      "4: loss: 3.7084577\n",
      "4: loss: 3.7055418\n",
      "4: loss: 3.7034438\n",
      "4: loss: 3.7009137\n",
      "4: loss: 3.698664\n",
      "4: loss: 3.6962278\n",
      "4: loss: 3.6932163\n",
      "4: loss: 3.690291\n",
      "4: loss: 3.6878052\n",
      "4: loss: 3.6849675\n",
      "4: loss: 3.682542\n",
      "4: loss: 3.6797163\n",
      "4: loss: 3.6769407\n",
      "4: loss: 3.6742663\n",
      "4: loss: 3.6712675\n",
      "4: loss: 3.6684973\n",
      "4: loss: 3.6660156\n",
      "4: loss: 3.663295\n",
      "4: loss: 3.6605742\n",
      "4: loss: 3.657871\n",
      "4: loss: 3.6550407\n",
      "4: loss: 3.6525047\n",
      "4: loss: 3.6496663\n",
      "4: loss: 3.646821\n",
      "4: loss: 3.6444278\n",
      "4: loss: 3.6426675\n",
      "4: loss: 3.6401134\n",
      "4: loss: 3.6383493\n",
      "4: loss: 3.635511\n",
      "4: loss: 3.6336937\n",
      "4: loss: 3.631861\n",
      "4: loss: 3.6303165\n",
      "4: loss: 3.6286027\n",
      "4: loss: 3.6267571\n",
      "4: loss: 3.6248562\n",
      "4: loss: 3.6232293\n",
      "4: loss: 3.6214461\n",
      "4: loss: 3.6195712\n",
      "4: loss: 3.6179934\n",
      "4: loss: 3.6162949\n",
      "4: loss: 3.6146312\n",
      "4: loss: 3.61278\n",
      "4: loss: 3.6112778\n",
      "4: loss: 3.6089325\n",
      "4: loss: 3.606911\n",
      "4: loss: 3.6040509\n"
     ]
    }
   ],
   "source": [
    "word_size = len(train_words_dict)\n",
    "batch_size = 10000\n",
    "embedding_size = 100\n",
    "\n",
    "context_pair = train_data\n",
    "\n",
    "inputs = [x[0] for x in context_pair]\n",
    "labels = [x[1] for x in context_pair]\n",
    "\n",
    "print(inputs[:10])\n",
    "print(labels[:10])\n",
    "\n",
    "emb_train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "emb_train_labels = tf.placeholder(tf.int32, shape=[batch_size,])\n",
    "\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([word_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "embed = tf.nn.embedding_lookup(embeddings, emb_train_inputs)\n",
    "\n",
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([word_size,embedding_size],\n",
    "                        stddev=1.0 / np.sqrt(embedding_size)))\n",
    "\n",
    "nce_biases = tf.Variable(tf.zeros([word_size]))\n",
    "\n",
    "prediction = tf.add(tf.matmul(embed, tf.transpose(nce_weights)), nce_biases)\n",
    "\n",
    "train_labels_vector = tf.one_hot(emb_train_labels,word_size)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=train_labels_vector))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "\n",
    "session = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "for iteration in range(0,5):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(int(len(inputs)/batch_size)):\n",
    "        feed_dict = {emb_train_inputs: inputs[i:i+batch_size], emb_train_labels: labels[i:i+batch_size]}\n",
    "        _, cur_loss,pred= session.run([optimizer, loss, prediction], feed_dict=feed_dict)\n",
    "        print('%s: loss: %s' %(iteration,cur_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keepdims = True))\n",
    "normalized_embeddings = embeddings/norm\n",
    "final_embeddings = normalized_embeddings.eval(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07144159 -0.02048308 -0.18019179 -0.17500421  0.04086721 -0.09685335\n",
      "  0.08644658  0.04763339  0.04065529  0.12077523 -0.17735511 -0.0179803\n",
      "  0.03753648  0.07633837 -0.12286776 -0.03059785 -0.07418384  0.01390408\n",
      " -0.0608811   0.11169364  0.04537367 -0.14498189  0.14247872  0.02494019\n",
      " -0.06840494  0.11314536  0.01086279  0.05335548  0.02231991  0.0577679\n",
      "  0.0774535  -0.01736649  0.1810616  -0.07715011  0.06870876 -0.15158826\n",
      " -0.07663272  0.05579735  0.09992765  0.08475394  0.00346566  0.00508084\n",
      "  0.17807399  0.08736991 -0.17152134 -0.05012795  0.10294256 -0.01720108\n",
      " -0.1170314  -0.17135015  0.06319413  0.16413617  0.10353651 -0.02969321\n",
      " -0.12981857 -0.1381176   0.02662657 -0.1492878   0.07284576  0.01076832\n",
      "  0.13512233  0.08191831  0.1385327  -0.09846588  0.05624063  0.11234745\n",
      " -0.10418097 -0.01521527 -0.09857556 -0.07638319  0.02226683  0.0150952\n",
      " -0.11109056 -0.03853367  0.17157394 -0.15474941 -0.0029797   0.00135735\n",
      " -0.01586181 -0.14694291 -0.03453944  0.12328327  0.05886955 -0.00343545\n",
      " -0.15956274  0.10492232  0.10346334 -0.0506623   0.06156529 -0.14450166\n",
      "  0.16889332  0.11640393 -0.15793107  0.05925966  0.13382001 -0.06377324\n",
      " -0.05374689  0.0982667  -0.17807004  0.03159278]\n"
     ]
    }
   ],
   "source": [
    "x = np.array(final_embeddings)\n",
    "#print(x[:10])\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27127, 100)\n",
      "(27127, 8)\n",
      "(27127, 108)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(embeddings.eval(session=session))\n",
    "\n",
    "of = np.array(other_feature)\n",
    "print(y.shape)\n",
    "print(of.shape)\n",
    "\n",
    "y_final = np.hstack((y,of))\n",
    "print(y_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nkind = defaultdict(list)\\nfor i in range(x.shape[0]):\\n    for j in range(len(test_labels_dict)):\\n        if label_of_words_dict[i] == j:\\n            kind[j].append(i)\\n\\nplt.figure(figsize=(5,5))\\nplt.scatter(x[kind[17],0],x[kind[17],1],alpha=0.6)\\n'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "kind = defaultdict(list)\n",
    "for i in range(x.shape[0]):\n",
    "    for j in range(len(test_labels_dict)):\n",
    "        if label_of_words_dict[i] == j:\n",
    "            kind[j].append(i)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(x[kind[17],0],x[kind[17],1],alpha=0.6)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442228\n",
      "442228\n"
     ]
    }
   ],
   "source": [
    "real_data = []\n",
    "for word in train_words:\n",
    "    real_data.append(y_final[train_words_dict[word]])\n",
    "\n",
    "print(len(real_data))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiangnan/.local/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = svm.SVC(C=1)\n",
    "clf.fit(real_data,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15  1 17  5 17  9 16 17 17 11 17 11 16 17 11]\n",
      "[5, 1, 6, 5, 10, 9, 16, 4, 17, 11, 17, 11, 16, 17, 11]\n"
     ]
    }
   ],
   "source": [
    "train_hat = clf.predict(real_data)\n",
    "#test_hat = clf.predict(test_data[:1000])\n",
    "#print(real_data[:10])\n",
    "print(train_hat[0:15])\n",
    "print(train_labels[0:15])\n",
    "#print(test_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15  1 17  5 17  9 16 17 17 11 17 11 16 17 11 17  1 17 16 17 12  2 17  1\n",
      "  5  7  5  7 16 17 13 16 17  1 17 16 17  1  5 10  5 16  5 17  1 15 10  6\n",
      "  9 16]\n",
      "[5, 1, 6, 5, 10, 9, 16, 4, 17, 11, 17, 11, 16, 17, 11, 17, 1, 10, 16, 17, 12, 2, 17, 1, 5, 7, 5, 10, 16, 4, 13, 16, 17, 1, 10, 16, 17, 1, 5, 10, 5, 16, 5, 4, 1, 5, 10, 6, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "print(train_hat[0:50])\n",
    "print(train_labels[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.8025891735401759\n"
     ]
    }
   ],
   "source": [
    "print('train accuracy:', accuracy_score(train_hat,train_labels))\n",
    "#print('test accuracy', accuracy_score(test_hat,test_labels[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
